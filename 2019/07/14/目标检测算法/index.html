<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">



<title>目标检测算法 | Young</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">XY&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">XY&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">目标检测算法</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xy</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">七月 14, 2019&nbsp;&nbsp;17:42:30</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>目标检测算法</p>
<ol>
<li><a href="https://blog.csdn.net/wopawn/article/details/52133338" target="_blank" rel="noopener">R-CNN论文详解</a> </li>
</ol>
<p><a href="https://blog.csdn.net/shenxiaolu1984/article/details/51066975" target="_blank" rel="noopener">R-CNN论文补充</a></p>
<p><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">RCNN原版论文</a></p>
<p><a href="https://www.cnblogs.com/pengsky2016/p/7921857.html" target="_blank" rel="noopener">RCNN论文翻译</a></p>
<ol start="2">
<li><a href="https://blog.csdn.net/wopawn/article/details/52463853" target="_blank" rel="noopener">Fast-RCNN论文详解</a></li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/24780433" target="_blank" rel="noopener">原始图片中的ROI如何映射到feature map</a></p>
<p><a href="https://blog.csdn.net/clearch/article/details/80224223" target="_blank" rel="noopener">目标检测中的正负样本</a></p>
<p><a href="https://blog.csdn.net/JNingWei/article/details/80337660" target="_blank" rel="noopener">Faster-RCNN框架图</a></p>
<ol start="2">
<li><a href="https://blog.csdn.net/Zachary_Co/article/details/78890768" target="_blank" rel="noopener">Faster-R-CNN论文及源码解读</a>  <a href="https://blog.csdn.net/shenxiaolu1984/article/details/51152614" target="_blank" rel="noopener">FasterRCNN 算法详解</a> </li>
</ol>
<p><a href="https://blog.csdn.net/sloanqin/article/details/51545125" target="_blank" rel="noopener">Faster-RCNN之RPN网络结构</a></p>
<ol start="3">
<li><a href="https://github.com/smallcorgi/Faster-RCNN_TF" target="_blank" rel="noopener">Faster-RCNNtensorflow源码</a> </li>
</ol>
<p>4.<a href="https://blog.csdn.net/antkillerfarm/article/details/79192385" target="_blank" rel="noopener">SSD,YOLOv2</a> </p>
<p><a href="https://blog.csdn.net/hrsstudy/article/details/70305791" target="_blank" rel="noopener">YOLOv1论文理解</a></p>
<p><a href="https://www.bilibili.com/video/av23354360?from=search&seid=11420157511609910495" target="_blank" rel="noopener">YOLOV1讲解视频</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/24954433" target="_blank" rel="noopener"><code>SDD模型知乎       专栏讲解</code></a></p>
<p><a href="https://blog.csdn.net/lanyuxuan100/article/details/70800246" target="_blank" rel="noopener">语义分割和实例分割的差别</a></p>
<p>5.<a href="https://blog.csdn.net/guoyunfei20/article/details/78723646" target="_blank" rel="noopener">选择性搜索（selective search)</a> </p>
<p>6.<a href="https://blog.csdn.net/liulina603/article/details/8291093" target="_blank" rel="noopener">HOG</a></p>
<p><a href="http://blog.51cto.com/13662721/2089735" target="_blank" rel="noopener">图像灰度化的三种常用方式及其实现</a></p>
<p><a href="http://www.52ml.net/15680.html" target="_blank" rel="noopener">DPM(Deformable Parts Model)</a></p>
<p><a href="https://blog.csdn.net/qq_22625309/article/details/72493223" target="_blank" rel="noopener">DPM</a></p>
<p><a href="https://blog.csdn.net/xwd18280820053/article/details/70674256" target="_blank" rel="noopener">准确率 召回率 F值</a></p>
<p>平均精度（Average Precision AP）<a href="https://www.cnblogs.com/eilearn/p/9071440.html" target="_blank" rel="noopener">一篇文章</a> <a href="https://blog.csdn.net/katherine_hsr/article/details/79266880" target="_blank" rel="noopener">二篇文章</a>  这两篇文章定义的AP还有本论文中定义的AP不一样 搞不明白。</p>
<p>[所以第三篇文章说它们是两种不同的计算方式]</p>
<p><a href="https://blog.csdn.net/u011771047/article/details/72872742/" target="_blank" rel="noopener">FCN中上采样,双线性插值，反卷积（转置卷积之间的关系）</a></p>
<p><a href="https://blog.csdn.net/u014722627/article/details/60574260/" target="_blank" rel="noopener">深度学习反卷积（转置卷积的理解）</a></p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic#convolution-arithmetic" target="_blank" rel="noopener">A technical report on convolution arithmetic in the context of deep learning</a></p>
<p>个人理解，博客中说,FCN论文中提到了双线性插值但在实现的时候用的是反卷积，反卷积的参数是可以被训练的，在 <a href="https://www.zhihu.com/question/63890195" target="_blank" rel="noopener">实现过程中双线性插值可以用反卷积的方式实现即使用双线性初始化卷积核</a>，然后把学习率设为0.而反卷积的具体实现方式 （程序内部）是采用转置卷积的方式实现。个人理解反卷积应该不属于上采样，而双线性插值才属于上采样</p>
<p><a href="https://blog.csdn.net/u014722627/article/details/60574260/" target="_blank" rel="noopener">计算机视觉中的三种上采样方式</a>（写的最清楚的一篇）</p>
<p><a href="https://blog.csdn.net/haoji007/article/details/78445298" target="_blank" rel="noopener">语义分割–FCN 算法中的一些细节–特征怎么融合</a></p>
<p><a href="https://blog.csdn.net/haoji007/article/details/78445298" target="_blank" rel="noopener">FCN学习</a></p>
<p><a href="https://blog.csdn.net/gyh_420/article/details/78570415" target="_blank" rel="noopener">FCN学习（写的较好）</a></p>
<p><a href="https://www.cnblogs.com/dmzhuo/p/6151434.html" target="_blank" rel="noopener">卷积中卷积方式same，valid</a></p>
<p>卷积的计算方式只有一种，(w-k+2p)/s+1</p>
<p>但是MATLAB和tensorflow的实现方式不一样</p>
<p><a href="https://blog.csdn.net/syyyy712/article/details/80272071" target="_blank" rel="noopener">TensorFlow卷积实现方式</a></p>
<p>matlab中卷积步长只有1，shape参数有三种不同的值，full,same,valid.</p>
<p><a href="https://www.cnblogs.com/hyb221512/p/9276621.html" target="_blank" rel="noopener">matlab中卷积函数</a></p>
<p>只有在卷积步长为1的情况下，same模式的输入输出大小才相同。</p>
<p><a href="https://blog.csdn.net/fdchao/article/details/52915716" target="_blank" rel="noopener">各种CNN网络结构及参数个数统计</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/43927696" target="_blank" rel="noopener">U-NET</a>:U-NET主要实现了一个U型网络，对图像数据做弹性形变，增加groundtruth,细胞边界附近的权值。</p>
<p>为了更好的实现对边界像素的处理，u-net对图像进行镜像处理。</p>
<h2 id="目标检测背景知识"><a href="#目标检测背景知识" class="headerlink" title="目标检测背景知识"></a>目标检测背景知识</h2><p>1.一图理解什么是图像分类，图像定位，目标检测 ，实例分割，语义分割<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsiwrq1xktj30fq07un19.jpg" alt="img"><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsiwtbk59sj30yl08xtbc.jpg" alt><br>像素级别的语义分割，对图像中的每个像素都划分出对应的类别，即实现像素级别的分类；  而类的具体对象，即为实例，那么实例分割不但要进行像素级别的分类，还需在具体的类别基础上区别开不同的实例。比如说图像有多个人甲、乙、丙，那边他们的语义分割结果都是人，而实例分割结果却是不同的对象。</p>
<h3 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h3><p><a href="https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/" target="_blank" rel="noopener">不平衡数据集的评价指标</a></p>
<p>(1)准确率（Accuracy）</p>
<p>准确率=预测正确的正反例个数/总的样本数</p>
<p>准确率一般用来评价模型的全局程度，不能包含很多信息，无法全面评价一个模型的性能。</p>
<p>(2)混淆矩阵(Confusion Matrix)</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fsixxv5s2wj30fe07pt9d.jpg" alt="confusion matrix"></p>
<p>混淆矩阵的横轴是模型预测的某个类别的样本数量的统计，而纵轴是数据真实标签的数量统计。对角线，表示模型预测和数据标签一致的数目，所以对角线之和除以测试集总数就是准确率。对角线上的数字越大越好，在可视化结果中颜色越深，说明模型在该类的预测准确率越高，如果按行来看，每行不在对角线位置的就是错误预测的类别。总的来说，我们希望对角线越高越好，非对角线越低越好。</p>
<p>（3）精确率（Precision）与召回率（Recall）</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fsjqo24bo1j30af0jqdh4.jpg" alt="img"></p>
<p>以下有几个概念需要先说明：</p>
<p>TP(True Positive): 真实为0，预测也为0</p>
<p>FN(False Negative): 真实为0，预测为1</p>
<p>FP(False Positive): 真实为1，预测为0</p>
<p>TN(True Negative): 真实为1，预测也为1</p>
<p>例：假设现在有这样一个测试集，测试集中的图片只有大雁和飞机两种图片组成，假设你的分类系统最终的目的是：能取出测试集中所有的飞机的图片，而不是大雁图片。</p>
<ul>
<li>True positives : <strong>正样本被正确识别为正样本</strong>，飞机的图片被正确的识别成了飞机。 </li>
<li>True negatives: <strong>负样本被正确识别为负样本</strong>，大雁的图片没有被识别出来，系统正确地认为它们是大雁。 </li>
<li>False positives: <strong>假的正样本，即负样本被错误识别为正样本</strong>，大雁的图片被错误地识别成了飞机。 </li>
<li>False negatives: <strong>假的负样本，即正样本被错误识别为负样本</strong>，飞机的图片没有被识别出来，系统错误地认为它们是大雁。</li>
</ul>
<p>　　<strong>Precision</strong>其实就是<strong>在识别出来的图片中，True positives所占的比率。</strong>也就是本假设中，所有被识别出来的飞机中，真正的飞机所占的比例。</p>
<p>   　<img src="https://ws4.sinaimg.cn/large/006tKfTcly1fsj4lkz2e1j307201rglf.jpg" alt="img"></p>
<p>　　<strong>Recall 是测试集中所有正样本样例中，被正确识别为正样本的比例</strong>。也就是本假设中，被正确识别出来的飞机个数与测试集中所有真实飞机的个数的比值。</p>
<p>　　<img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsj4m1rxprj304q01jjr6.jpg" alt="img"></p>
<p>　　<strong>Precision-recall 曲线：</strong>改变识别阈值，使得系统依次能够识别前K张图片，阈值的变化同时会导致Precision与Recall值发生变化，从而得到曲线。</p>
<p>　　如果一个分类器的性能比较好，那么它应该有如下的表现：在Recall值增长的同时，Precision的值保持在一个很高的水平。而性能比较差的分类器可能会损失很多Precision值才能换来Recall值的提高。通常情况下，文章中都会使用Precision-recall曲线，来显示出分类器在Precision与Recall之间的权衡。</p>
<p>（4）平均精度（Average-Precision,AP）与 mean Average Precision(mAP)</p>
<p>AP就是Precision-recall曲线下面的面积，通常来说一个越好的分类器，AP值越高。在实际操作中一般是用11点插值法来计算。</p>
<p>mAP是多个类别AP的平均值。这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP大小一定在[0,1]区间，越大越好，该指标是目标检测算法中最重要的一个。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsjr05i6dtj30i60cowhc.jpg" alt="img"></p>
<p>（5）IoU</p>
<p>IoU这一值，可以理解为系统预测出来的框与groundtruth的重合程度。计算方法即检测结果Detection Result与Ground Truth的交集比上它们的并集，即为检测的准确率。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsjrmodhqij309701hglh.jpg" alt="img"></p>
<p>（6）ROC（Receiver Operating Characteristic）曲线与AUC(Area Under Curve)</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fsjrt2dtylj309v09c76t.jpg" alt="img"></p>
<p>ROC曲线（受试者工作特征曲线）：</p>
<ul>
<li><p>横坐标：假正率（False positive rate,FDR）,FDR = FP/[FP+TN],代表所有负样本中错误预测为正样本的概率，假警报率；</p>
</li>
<li><p>纵坐标：真正率（True positive rate, TPR）, TPR = TP/[TP+FN],代表所有正样本中预测正确的概率，命中率。</p>
<p>对角线对应于随机猜测模型，而（0，1）对应于所有正例排在所有反例之前的理想模型，就是全部预测正确。曲线越接近左上角，分类器性能越好。</p>
<p>ROC 曲线有个很好的特性：当测试集中的正负样本分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡的现象，即负样本比正样本多很多，而且测试数据中的正负样本的分布也可能随着时间变化。</p>
<p><strong>ROC曲线绘制：</strong></p>
<p>　　（1）根据每个测试样本属于正样本的概率值从大到小排序；</p>
<p>　　（2）从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本；</p>
<p>　　（3）每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。 </p>
<p> 　　当我们将threshold设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当threshold取值越多，ROC曲线越平滑。</p>
</li>
</ul>
<p><strong>AUC（Area Under Curve）即为ROC曲线下的面积。AUC越接近于1，分类器性能越好。</strong></p>
<p> 　　<strong>物理意义：</strong>首先AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。</p>
<p>　　计算公式：就是求曲线下矩形面积。</p>
<p>　　<img src="https://ws3.sinaimg.cn/large/006tNc79ly1fsjsiyjl2mj307o01vwea.jpg" alt="img"></p>
<p>（7）非极大值抑制（NMS）</p>
<p>Non-Maximum Suppression 就是需要根据score矩阵和region的坐标信息，从中找到置信度较高的bounding box,对于有重叠在一起的 的预测框，只保留得分最高的那个。··</p>
<p>​      1.NMS计算出每个bounding box的面积，然后根据score进行排序，把score中最大的bounding box 作为队列中首个要比较的对象；</p>
<p>​     2.计算其余boundingbox与当前最大score的box的IOU，去除IOU大于设定的阈值的bounding box，保留晓得IOU预测框。</p>
<p>​    3.然后重复上面过程，直到候选bounding box为空</p>
<p>NMS一次处理一个类别。有N个类别,就需要N次执行。</p>
<p>近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。而另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。</p>
<h3 id="RCNN解析"><a href="#RCNN解析" class="headerlink" title="RCNN解析"></a>RCNN解析</h3><p>Region CNN(RCNN)（2014）可以说是利用深度学习进行目标检测的开山之作。作者Ross Grishick(rbg大神)多次在PASCAL VOC的目标检测竞赛中折桂，2010年更带领团队获得终身成就奖，如今供职于Facebook旗下的FAIR。<a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="noopener">RCNN</a>, <a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">Fast RCNN</a>, <a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="noopener">Faster RCNN</a>代表当下目标检测的前沿水平，<strong>在github都给出了基于Caffe的源码</strong>。</p>
<h4 id="1-RCNN算法流程"><a href="#1-RCNN算法流程" class="headerlink" title="1.RCNN算法流程"></a>1.RCNN算法流程</h4><p>RCNN算法分为4个步骤：</p>
<ul>
<li><p>一张图像采用Selective Search 方法生成1K到2k个候选区域</p>
</li>
<li><p><strong>对每个候选区域</strong> ，使用卷积神经网络提取特征。</p>
</li>
<li><p>特征送入每一类的SVM分类器，判别是否属于该类。</p>
</li>
<li><p>使用回归器精细修正候选框位置。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fsjy2pg7wnj30k505tq50.jpg" alt="这里写图片描述"></p>
<h4 id="2-候选区域的生成（Selective-Search算法）"><a href="#2-候选区域的生成（Selective-Search算法）" class="headerlink" title="2.候选区域的生成（Selective Search算法）"></a>2.候选区域的生成（Selective Search算法）</h4><p>在目标检测时，为了定位到目标的具体位置，通常会把图像分成许多子块（sub-regions / patches），然后把子块作为输入，送到目标识别的模型中。分子块的最直接方法叫滑动窗口法（sliding window approach）。滑动窗口的方法就是按照子块的大小在整幅图像上穷举所有子图像块。这种方法产生的数据量想想都头大。和滑动窗口法相对的是另外一类基于区域（region proposal）的方法。selective search就是其中之一！</p>
<p>（1）Selective Search 算法流程</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fsjzof4qdij30jn0grjwj.jpg" alt="img"></p>
<p>step0：生成区域集R，具体参见论文<a href="http://blog.csdn.net/guoyunfei20/article/details/78727972" target="_blank" rel="noopener">《Efficient Graph-Based Image Segmentation》</a></p>
<p>step1：计算区域集R里每个相邻区域的相似度S={s1,s2,…}<br>step2：找出相似度最高的两个区域，将其合并为新集，添加进R<br>step3：从S中移除所有与step2中有关的子集<br>step4：计算新集与所有相邻区域的相似度<br>step5：跳至step2，直至S为空</p>
<h4 id="3-相似度计算"><a href="#3-相似度计算" class="headerlink" title="3.相似度计算"></a><strong>3.相似度计算</strong></h4><p>论文考虑了颜色、纹理、尺寸和空间交叠这4个参数。</p>
<p>3.1、颜色相似度（color similarity）<br>将色彩空间转为HSV，每个通道下以bins=25计算直方图，这样每个区域的颜色直方图有25*3=75个区间。 对直方图除以区域尺寸做归一化后使用下式计算相似度：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fsk0dlemrcj308h020t8m.jpg" alt="img"></p>
<p>3.2、纹理相似度（texture similarity）<br>论文采用方差为1的高斯分布在8个方向做梯度统计，然后将统计结果（尺寸与区域大小一致）以bins=10计算直方图。直方图区间数为8*3*10=240（使用RGB色彩空间）。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fsk0cxgl7gj306n01fdfl.jpg" alt="img"></p>
<p>其中,$t_i^k$是直方图中第$k^{th}$个bin的值。</p>
<p>3.3、尺寸相似度（size similarity）</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsk0d4vy80j307u016dfl.jpg" alt="img"></p>
<p>保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域。</p>
<p>例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh。</p>
<p>3.4、交叠相似度（shape compatibility measure）</p>
<p>这里主要是为了衡量两个区域是否更加吻合，其指标是合并后的区域的Bounding Box越小，其吻合度越高。其计算方式：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsk0cb25ydj30ap016jr5.jpg" alt="img"></p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsk0ckvlbrj30ag04zdfx.jpg" alt="img"></p>
<p>3.5、最终的相似度</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fsk0dfpz4gj30fx00jjr5.jpg" alt="img"></p>
</li>
</ul>
<p>  <strong>多样化和后处理</strong></p>
<p>  为尽可能不遗漏候选区域，上述操作在多个颜色空间中同时进行（RGB,HSV,Lab等）。在一个颜色空间中，使用上述四条规则的不同组合进行合并。所有颜色空间与所有规则的全部结果，在去除重复后，都作为候选区域输出。</p>
<p>  作者提供了Selective Search的<a href="http://koen.me/research/selectivesearch/" target="_blank" rel="noopener">源码</a>，内含较多.p文件和.mex文件，难以细查具体实现。</p>
<h4 id="4-特征提取阶段（CNN网络）"><a href="#4-特征提取阶段（CNN网络）" class="headerlink" title="4.特征提取阶段（CNN网络）"></a>4.特征提取阶段（CNN网络）</h4><p>  4.1预处理</p>
<p>  论文中采用AlexNet CNN网络进行特征提取，为了适应AlexNet网络的输入图像大小：227x227,故将所有RegionProposal变形为227x227.</p>
<p>  那么问题来了，如何进行变形操作呢？</p>
<p>  ① 考虑context【图像中context指RoI周边像素】的各向同性变形，建议框像周围像素扩充到227×227，若遇到图像边界则用建议框像素均值填充，下图第二列；<br>  ② 不考虑context的各向同性变形，直接用建议框像素均值填充至227×227，下图第三列；<br>  ③ 各向异性变形，简单粗暴对图像就行缩放至227×227，下图第四列；<br>  ④ 变形前先进行边界像素填充【padding】处理，即向外扩展建议框边界，以上三种方法中分别采用padding=0下图第一行，padding=16下图第二行进行处理；</p>
<p>  经过作者一系列实验表明采用padding=16的各向异性变形即下图第二行第三列效果最好，能使mAP提升3-5%。</p>
<p>  <img src="https://ws1.sinaimg.cn/large/0069RVTdly1ftzqkph9p3j30vy0iaqof.jpg" alt="屏幕快照 2018-08-06 上午9.43.30"> </p>
<p>  4.2网络结构</p>
<p>  <img src="https://ws3.sinaimg.cn/large/006tNc79ly1fskvwzrzp2j30ia05bmxa.jpg" alt="这里写图片描述"></p>
<p>  pool5层的size是6x6x256,即有256种表示的不同特征，这相当于原始227x227的图片中有256种195x195的感受视野。</p>
<p>  随后将原始网络中4096x1000的全连接层替换为4096x21的全连接层</p>
<h4 id="5-非极大抑制"><a href="#5-非极大抑制" class="headerlink" title="5.非极大抑制"></a>5.非极大抑制</h4><p>  在测试过程完成到第4步后，获得2000x20维矩阵表示每个建议框是某个物体类别的得分情况，此时会遇到如下图所示情况，同一个车辆目标会被多个建议框包围，这时需要非极大值抑制操作去除得分较低的候选框以减少重叠框。</p>
<p>  <img src="https://ws3.sinaimg.cn/large/006tNc79ly1fskws8t6l5j30bk07r101.jpg" alt="这里写图片描述"></p>
<p> 具体怎么做呢？</p>
<p>(1)对2000x20维的矩阵每列按从大到小进行排序；即针对每个类的2000个建议框进行排序。</p>
<p>(2)从每列最大的得分简易框开始，分别与该列后面的得分建议框进行IoU计算，若IoU&gt;阈值，则剔除得分较小的建议框，否则认为图像中存在多个同一类物体；</p>
<p>(3) 从每列次大的得分建议框开始，重复步骤(2)。</p>
<p>(4)重复步骤(3)直到遍历完该列所有建议框；</p>
<p>(5)遍历完2000x20维的矩阵所有列，即所有物体种类都做一遍非极大值抑制；</p>
<p>(6)最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框。</p>
<h4 id="6-回归器"><a href="#6-回归器" class="headerlink" title="6.回归器"></a>6.回归器</h4><p>首先要明确目标检测不仅要对目标进行识别，还要完成定位任务，所以最终获得的bounding-box也决定了目标检测的精度。</p>
<p>这里先解释一下什么叫定位精度：定位精度可以用算法得出的物体检测框与实际标注的物体边界框的IoU值来近似表示。</p>
<p>如下图所示，绿色框为实际标准的卡宴车辆框，即Ground Truth；黄色框为selective search算法得出的建议框，即Region Proposal。即使黄色框中物体被分类器识别为卡宴车辆，但是由于绿色框和黄色框IoU值并不大，所以最后的目标检测精度并不高。采用回归器是为了对建议框进行校正，使得校正后的Region Proposal与Groundtruth更接近， 以提高最终的检测精度。论文中采用bounding-box回归使mAP提高了3~4%。   <img src="https://ws1.sinaimg.cn/large/006tNc79ly1fsky0mgum8j30bk07sahj.jpg" alt="这里写图片描述"></p>
<p>那么问题来了，回归器应该如何设计？</p>
<p>首先要明确目标检测不仅是要对目标进行识别，还要完成定位任务，所以最终获得的bounding-box也决定了目标检测的精度。<br>这里先解释一下什么叫定位精度：定位精度可以用算法得出的物体检测框与实际标注的物体边界框的IoU值来近似表示。</p>
<p>如下图所示，绿色框为实际标准的卡宴车辆框，即Ground Truth；黄色框为selective search算法得出的建议框，即Region Proposal。即使黄色框中物体被分类器识别为卡宴车辆，但是由于绿色框和黄色框IoU值并不大，所以最后的目标检测精度并不高。采用回归器是为了对建议框进行校正，使得校正后的Region Proposal与selective search更接近， 以提高最终的检测精度。论文中采用bounding-box回归使mAP提高了3~4%。 </p>
<p>那么问题来了，回归器如何设计呢？ </p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fskybgau5ij309608it8s.jpg" alt="这里写图片描述"><br>如上图，黄色框口P表示建议框Region Proposal，绿色窗口G表示实际框Ground Truth，红色窗口$\widehat{G}$表示Region Proposal进行回归后的预测窗口，现在的目标是找到P到$\widehat{G}$的线性变换【当Region Proposal与Ground Truth的IoU&gt;0.6时可以认为是线性变换】，使得$\widehat{G}$与G越相近，这就相当于一个简单的可以用最小二乘法解决的线性回归问题，具体往下看。<br>让我们先来定义P窗口的数学表达式：$P^i=(P^i_x，P^i_y，P^i_w，P^i_h)$，其中$(P^i_x，P^i_y)$表示第一个i窗口的中心点坐标，$P^i_w，P^i_h$分别为第i个窗口的宽和高；G窗口的数学表达式为：$Gi=(G^i_x，G^i_y，G^i_w，G^i_h)$；$\widehat{G}$窗口的数学表达式为：$\widehat{G}^i=(\widehat{G}^i_x，\widehat{G}^i_y，\widehat{G}^i_w，\widehat{G}^i_h)$。以下省去i上标。<br>这里定义了四种变换函数，$d_x(P)，d_y(P)，d_w(P)，d_h(P)$。$d_x(P)$和$d_y(P)$通过平移对x和y进行变化，$d_w(P)$和$d_h(P)$通过缩放对w和h进行变化，即下面四个式子所示： </p>
<p>$G^x=P_wd_x(P)+P_x$                  (1)</p>
<p>$G^y=P_hd_y(P)+P_y$                   (2)</p>
<p>$G^w=P_wexp(d_w(P))$                  (3)</p>
<p>$G^h=P_hexp(d_h(P))$                   (4)</p>
<p>每一个$d_∗(P)$【*表示x，y，w，h】都是一个AlexNet CNN网络Pool5层特征$\phi5(P)$的线性函数，即$d_∗(P)=w^T_∗\phi5(P)$ ，这里$w^T_∗$就是所需要学习的回归参数。损失函数即为：</p>
<p>$目标函数=argmin\sum_{i=0}^N(t^i_∗−\widehat{w}^T_∗\phi5(Pi))^2+λ||\widehat{w}_∗||^2$                                  (5)</p>
<p>损失函数中加入正则项$λ||w^∗||^2$   是为了避免归回参数$w^T_∗$过大。其中，回归目标$t_∗$由训练输入对(P，G)按下式计算得来：$t_x=(G_x−P_x)/P_w$(6)</p>
<p>$t_y=(G_y−P_y)/P_h$  (7) </p>
<p>$t_w=log(G_w/P_w)$    (8)</p>
<p>$t_h=log(G_h/P_h)$     (9)</p>
<p>①构造样本对。为了提高每类样本框回归的有效性，对每类样本都仅仅采集与Ground Truth相交IoU最大的Region Proposal，并且IoU&gt;0.6的Region Proposal作为样本对$(P_i，G_i)$，一共产生20对样本对【20个类别】； ?可以产生20个样本对吗？<br>②每种类型的回归器单独训练，输入该类型样本对N个：${(P_i,G_i)} i=1⋯N$以及$P^i_{i=1⋯N}$所对应的AlexNet CNN网络Pool5层特征$\phi5(P^i)<em>{i=1⋯N}$；<br>③利用(6)-(9)式和输入样本对${(P_i,G_i)}</em>{i=1⋯N}$计算$t^i_{∗i=1⋯N}$；<br>④利用$\phi5(P_i)<em>{i=1⋯N}$$和$$^i</em>{∗i=1⋯N}$，根据损失函数(5)进行回归，得到使损失函数最小的参数$w^T_∗$。</p>
<h4 id="7-训练过程"><a href="#7-训练过程" class="headerlink" title="7.训练过程"></a>7.<strong>训练过程</strong></h4><ol>
<li><p>有监督预训练</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>ILSVRC2012</td>
</tr>
<tr>
<td>负样本</td>
<td>ILSVRC2012</td>
</tr>
</tbody></table>
<p>ILSVRC样本集上仅有图像类别标签，没有图像物体位置标注；<br>采用AlexNet CNN网络进行有监督预训练，学习率=0.01；<br>该网络输入为227×227的ILSVRC训练集图像，输出最后一层为4096维特征-&gt;1000类的映射，训练的是网络参数。</p>
</li>
<li><p>特定样本下的微调</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>Ground Truth+与Ground Truth相交IoU&gt;0.5的建议框【由于Ground Truth太少了】</td>
</tr>
<tr>
<td>负样本</td>
<td>与Ground Truth相交IoU≤0.5的建议框</td>
</tr>
</tbody></table>
<p>PASCAL VOC 2007样本集上既有图像中物体类别标签，也有图像中物体位置标签；<br>采用训练好的AlexNet CNN网络进行PASCAL VOC 2007样本集下的微调，学习率=0.001【0.01/10为了在学习新东西时不至于忘记之前的记忆】；<br>mini-batch为32个正样本和96个负样本【由于正样本太少】；<br>该网络输入为建议框【由selective search而来】变形后的227×227的图像，修改了原来的1000为类别输出，改为21维【20类+背景】输出，训练的是网络参数。</p>
</li>
<li><p>SVM训练</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>Ground Truth</td>
</tr>
<tr>
<td>负样本</td>
<td>与Ground Truth相交IoU＜0.3的建议框</td>
</tr>
</tbody></table>
<p>由于SVM是二分类器，需要为每个类别训练单独的SVM；<br>SVM训练时输入正负样本在AlexNet CNN网络计算下的4096维特征，输出为该类的得分，训练的是SVM权重向量；<br>由于负样本太多，采用hard negative mining的方法在负样本中选取有代表性的负样本。</p>
</li>
<li><p>Bounding-box regression训练</p>
<table>
<thead>
<tr>
<th>样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>与Ground Truth相交IoU最大的Region Proposal，并且IoU&gt;0.6的Region Proposal</td>
</tr>
</tbody></table>
<p>输入数据为某类型样本对N个：{(Pi,Gi)}i=1⋯N以及Pii=1⋯N所对应的AlexNet CNN网络Pool5层特征ϕ5(Pi)i=1⋯N，输出回归后的建议框Bounding-box，训练的是dx(P)，dy(P)，dw(P)，dh(P)四种变换操作的权重向量。具体见前面分析。</p>
<h4 id="8-解释分析"><a href="#8-解释分析" class="headerlink" title="8.解释分析"></a>8.解释分析</h4><ol>
<li><p>什么叫有监督预训练？为什么要进行有监督预训练？</p>
<p>有监督预训练也称之为迁移学习，举例说明：若有大量标注信息的人脸年龄分类的正负样本图片，利用样本训练了CNN网络用于人脸年龄识别；现在要通过人脸进行性别识别，那么就可以去掉已经训练好的人脸年龄识别网络CNN的最后一层或几层，换成所需要的分类层，前面层的网络参数直接使用为初始化参数，修改层的网络参数随机初始化，再利用人脸性别分类的正负样本图片进行训练，得到人脸性别识别网络，这种方法就叫做有监督预训练。这种方式可以很好地解决小样本数据无法训练深层CNN网络的问题，我们都知道小样本数据训练很容易造成网络过拟合，但是在大样本训练后利用其参数初始化网络可以很好地训练小样本，这解决了小样本训练的难题。<br>这篇文章最大的亮点就是采用了这种思想，ILSVRC样本集上用于图片分类的含标注类别的训练集有1millon之多，总共含有1000类；而PASCAL VOC 2007样本集上用于物体检测的含标注类别和位置信息的训练集只有10k，总共含有20类，直接用这部分数据训练容易造成过拟合，因此文中利用ILSVRC2012的训练集先进行有监督预训练。</p>
</li>
<li><p>ILSVRC 2012与PASCAL VOC 2007数据集有冗余吗？</p>
<p>即使图像分类与目标检测任务本质上是不同的，理论上应该不会出现数据集冗余问题，但是作者还是通过两种方式测试了PASCAL 2007测试集和ILSVRC 2012训练集、验证集的重合度：第一种方式是检查网络相册IDs，4952个PASCAL 2007测试集一共出现了31张重复图片，0.63%重复率；第二种方式是用GIST描述器匹配的方法，4952个PASCAL 2007测试集一共出现了38张重复图片【包含前面31张图片】，0.77%重复率，这说明PASCAL 2007测试集和ILSVRC 2012训练集、验证集基本上不重合，没有数据冗余问题存在。</p>
</li>
<li><p>可以不进行特定样本下的微调吗？可以直接采用AlexNet CNN网络的特征进行SVM训练吗？</p>
<p>文中设计了没有进行微调的对比实验，分别就AlexNet CNN网络的pool5、fc6、fc7层进行特征提取，输入SVM进行训练，这相当于把AlexNet CNN网络当做万精油使用，类似HOG、SIFT等做特征提取一样，不针对特征任务。实验结果发现f6层提取的特征比f7层的mAP还高，pool5层提取的特征与f6、f7层相比mAP差不多；<br>在PASCAL VOC 2007数据集上采取了微调后fc6、fc7层特征较pool5层特征用于SVM训练提升mAP十分明显；<br>由此作者得出结论：不针对特定任务进行微调，而将CNN当成特征提取器，pool5层得到的特征是基础特征，类似于HOG、SIFT，类似于只学习到了人脸共性特征；从fc6和fc7等全连接层中所学习到的特征是针对特征任务特定样本的特征，类似于学习到了分类性别分类年龄的个性特征。</p>
</li>
<li><p>为什么微调时和训练SVM时所采用的正负样本阈值【0.5和0.3】不一致？</p>
<p>微调阶段是由于CNN对小样本容易过拟合，需要大量训练数据，故对IoU限制宽松：Ground Truth+与Ground Truth相交IoU&gt;0.5的建议框为正样本，否则为负样本；<br>SVM这种机制是由于其适用于小样本训练，故对样本IoU限制严格：Ground Truth为正样本，与Ground Truth相交IoU＜0.3的建议框为负样本。</p>
</li>
<li><p>为什么不直接采用微调后的AlexNet CNN网络最后一层SoftMax进行21分类【20类+背景】？</p>
<p>因为微调时和训练SVM时所采用的正负样本阈值不同，微调阶段正样本定义并不强调精准的位置，而SVM正样本只有Ground Truth；并且微调阶段的负样本是随机抽样的，而SVM的负样本是经过hard negative mining方法筛选的；导致在采用SoftMax会使PSACAL VOC 2007测试集上mAP从54.2%降低到50.9%。</p>
</li>
</ol>
<h4 id="9-结果怎么样"><a href="#9-结果怎么样" class="headerlink" title="9.结果怎么样"></a>9.结果怎么样</h4><ol>
<li>PASCAL VOC 2010测试集上实现了53.7%的mAP；</li>
<li>PASCAL VOC 2012测试集上实现了53.3%的mAP；</li>
<li>计算Region Proposals和features平均所花时间：13s/image on a GPU；53s/image on a CPU。</li>
</ol>
<h4 id="10-还存在什么问题"><a href="#10-还存在什么问题" class="headerlink" title="10. 还存在什么问题"></a>10. 还存在什么问题</h4><ol>
<li><p>很明显，最大的缺点是对一张图片的处理速度慢，这是由于一张图片中由selective search算法得出的约2k个建议框都需要经过变形处理后由CNN前向网络计算一次特征，这其中涵盖了对一张图片中多个重复区域的重复计算，很累赘；</p>
</li>
<li><p>知乎上有人说R-CNN网络需要两次CNN前向计算，第一次得到建议框特征给SVM分类识别，第二次对非极大值抑制后的建议框再次进行CNN前向计算获得Pool5特征，以便对建议框进行回归得到更精确的bounding-box，这里文中并没有说是怎么做的，博主认为也可能在计算2k个建议框的CNN特征时，在硬盘上保留了2k个建议框的Pool5特征，虽然这样做只需要一次CNN前向网络运算，但是耗费大量磁盘空间；</p>
</li>
<li><p>训练时间长，虽然文中没有明确指出具体训练时间，但由于采用RoI-centric sampling【从所有图片的所有建议框中均匀取样】进行训练，那么每次都需要计算不同图片中不同建议框CNN特征，无法共享同一张图的CNN特征，训练速度很慢；</p>
</li>
<li><p>整个测试过程很复杂，要先提取建议框，之后提取每个建议框CNN特征，再用SVM分类，做非极大值抑制，最后做bounding-box回归才能得到图片中物体的种类以及位置信息；同样训练过程也很复杂，ILSVRC 2012上预训练CNN，PASCAL VOC 2007上微调CNN，做20类SVM分类器的训练和20类bounding-box回归器的训练；这些不连续过程必然涉及到特征存储、浪费磁盘空间等问题。</p>
</li>
</ol>
</li>
</ol>
<h3 id="Fast-RCNN解释"><a href="#Fast-RCNN解释" class="headerlink" title="Fast-RCNN解释"></a>Fast-RCNN解释</h3><h4 id="1-如何解决RCNN中存在的问题"><a href="#1-如何解决RCNN中存在的问题" class="headerlink" title="1.如何解决RCNN中存在的问题"></a>1.如何解决RCNN中存在的问题</h4><p>Fast R-CNN的网络结构如下图所示。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fsprue6nhcj30rr063mxg.jpg" alt="这里写图片描述"></p>
<ol>
<li>任意size图片输入CNN网络，经过若干卷积层与池化层，得到特征图；</li>
<li>在任意size图片上采用selective search算法提取约2k个建议框；</li>
<li>根据原图中建议框到特征图映射关系，在特征图中找到每个建议框对应的特征框【深度和特征图一致】，并在RoI池化层中将每个特征框池化到H×W【VGG-16网络是7×7】的size；</li>
<li>固定H×W【VGG-16网络是7×7】大小的特征框经过全连接层得到固定大小的特征向量；</li>
<li>第4步所得特征向量经由各自的全连接层【由SVD分解实现】，分别得到两个输出向量：一个是softmax的分类得分，一个是Bounding-box窗口回归；</li>
<li>利用窗口得分分别对每一类物体进行非极大值抑制剔除重叠“”建议框，最终得到每个类别中回归修正后的得分最高的窗口。</li>
</ol>
<hr>
<p><strong>解释分析</strong></p>
<ol>
<li><p>在RCNN中，用selectivesearch算法得到的regionPropoals都要进行一次特征提取操作，由于这些RegionPropoals存在很多重叠区域，所以存在大量的冗余特征提取操作。实际上我们只需要对整张图片进行一次特征提取操作，在提取到整张图片的特征图后，找出每个RegionPropoals对应的特征区域即可。这样避免冗余操作，节省大量时间。</p>
</li>
</ol>
<p>2.为什么要将每个建议的特征框池化到HxW的size,如何实现？</p>
<p>像AlexNet CNN等网络在提取特征过程中对图像的大小并无要求，只是在提取完特征进行全连接操作的时候才需要固定特征尺寸，利用这一点，Fast R-CNN可输入任意size图片，并在全连接操作前加入RoI池化层，将建议框对应特征图中的特征框池化到H×W 的size，以便满足后续操作对size的要求；</p>
<p>具体如何实现呢?<br>首先假设建议框对应特征图中的特征框大小为h×w，将其划分H×W个子窗口，每个子窗口大小为h/H×w/W，然后对每个子窗口采用max pooling下采样操作，每个子窗口只取一个最大值，则特征框最终池化为H×W的size【特征框各深度同理】，这将各个大小不一的特征框转化为大小统一的数据输入下一层。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fspsrdm6c2j315s0dv461.jpg" alt="这里写图片描述"></p>
<h2 id="ROI-Pooling的输入"><a href="#ROI-Pooling的输入" class="headerlink" title="ROI Pooling的输入"></a><strong>ROI Pooling的输入</strong></h2><p>输入有两部分组成： </p>
<ol>
<li>特征图：指的是上图中所示的特征图，在Fast RCNN中，它位于RoI Pooling之前，在Faster RCNN中，它是与RPN共享那个特征图，通常我们常常称之为“share_conv”； </li>
<li>rois：在Fast RCNN中，指的是Selective Search的输出；在Faster RCNN中指的是RPN的输出，一堆矩形候选框框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对<strong>原图</strong>的（神经网络最开始的输入）</li>
</ol>
<h2 id="ROI-Pooling的输出"><a href="#ROI-Pooling的输出" class="headerlink" title="ROI Pooling的输出"></a><strong>ROI Pooling的输出</strong></h2><p>输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel * w * h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框；</p>
<h2 id="ROI-Pooling的过程"><a href="#ROI-Pooling的过程" class="headerlink" title="ROI Pooling的过程"></a><strong>ROI Pooling的过程</strong></h2><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fspt100ipvj31kw23vb29.jpg" alt="这里写图片描述"></p>
<p>如图所示，我们先把roi中的坐标映射到feature map上，映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标后，我们使用Pooling得到输出；由于输入的图片大小不一，所以这里我们使用的类似Spp Pooling，在Pooling的过程中需要计算Pooling后的结果对应到feature map上所占的范围，然后在那个范围中进行取max或者取average。</p>
<p><a href="https://blog.csdn.net/auto1993/article/details/78514071" target="_blank" rel="noopener">ROIpooling层正向传播过程</a></p>
<p>3.为什么采用SVD实现Fast R-CNN网络的全连接层，具体如何实现？</p>
<p>SVD分解加速全连接层的计算</p>
<p>（1）物体分类和窗口回归都是用全连接层实现的，</p>
<p>假设全连接层的输入数据为x,输出数据为y,权值矩阵为W,尺寸为uxv,那么该层的全连接计算为：</p>
<p>​                                                                  $y=Wx$</p>
<p>计算复杂度为uxv.</p>
<p>(2)若将W进行<a href="https://blog.csdn.net/xiaocong1990/article/details/54909126/" target="_blank" rel="noopener">SVD分解</a>，并且用前t个特征值近似替代，</p>
<p>$W=U∑V^T≈U(u,1:t)⋅∑(1:t,1:t)⋅V(v,1:t)^T$</p>
<p>那么原来的前向传播分解成两步:</p>
<p>$y=Wx=U⋅(∑⋅V^T)⋅x=U⋅z$</p>
<p>计算复杂度为u×t+v×t，若t&lt;min(u,v)，则这种分解会大大减少计算量；</p>
<p>在实现时，相当于把一个全连接层拆分为两个全连接层，第一个全连接层不含偏置，第二个全连接层含偏置；实验表明，SVD分解全连接层能使mAP只下降0.3%的情况下提升30%的速度，同时该方法也不必再执行额外的微调操作。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fsr6a8h06nj30de05kdfv.jpg" alt="这里写图片描述"></p>
<p> 4.文中采用selectivesearch 算法 仅提取2000个候选区域，那候选区域越多越好吗？</p>
<p>文中利用selective search算法提取1k~10k中10种数目【1k，2k…】的候选区域进行训练测试，发现随着候选区域个数的增加，mAP成先增加后缓慢下滑的趋势，这表明更多的候选区域会有损精度；与此同时，作者也做了召回率【所谓召回率即候选区域为真的窗口与Ground Truth的比值【IoU大于阈值即为真】】分析实验，发现随着候选区域个数的增加，召回率并没有和mAP成很好的相关性，而是一直不断增加，也就是说更高的召回率并不意味着更高的mAP；文中也以selective search算法提取的2k个候选区域为基础，每次增加1000 × {2, 4, 6, 8, 10, 32, 45}个密集box【滑动窗口方法】进行训练测试，发现mAP比只有selective search方法的2k候选区域下降幅度更大，最终达到53%。</p>
<p>5.如何处理尺度不变性问题？即如何使24×24和1080×720的车辆同时在一个训练好的网络中都能正确识别？  </p>
<p>文中提及两种方式处理：brute-force（单一尺度）和image pyramids（多尺度）。单一尺度直接在训练和测试阶段将image定死为某种scale，直接输入网络训练就好，然后期望网络自己能够学习到scale-invariance的表达；多尺度在训练阶段随机从图像金字塔【缩放图片的scale得到，相当于扩充数据集】中采样训练，测试阶段将图像缩放为金字塔中最为相似的尺寸进行测试；</p>
<p>可以看出，多尺度应该比单一尺度效果好。作者在5.2节对单一尺度和多尺度分别进行了实验，不管哪种方式下都定义图像短边像素为s，单一尺度下s=600【维持长宽比进行缩放】，长边限制为1000像素；多尺度s={480,576,688,864,1200}【维持长宽比进行缩放】，长边限制为2000像素，生成图像金字塔进行训练测试；实验结果表明AlexNet【S for small】、VGG_CNN_M_1024【M for medium】下单一尺度比多尺度mAP差1.2%~1.5%，但测试时间上却快不少，VGG-16【L for large】下仅单一尺度就达到了66.9%的mAP【由于GPU显存限制多尺度无法实现】，该实验证明了深度神经网络善于直接学习尺度不变形，对目标的scale不敏感。</p>
<p>6.为什么不沿用R-CNN中的形式继续采用SVM进行分类？  </p>
<p>为什么R-CNN中采用SVM分类而不直接用CNN网络输出端进行分类已经在R-CNN博客中说明，针对Fast R-CNN，文中分别进行实验并对比了采用SVM和采用softmax的mAP结果，不管AlexNet【S for small】、VGG_CNN_M_1024【M for medium】、VGG-16【L for large】中任意网络，采用softmax的mAP都比采用SVM的mAP高0.1%~0.8%，这是由于softmax在分类过程中引入了类间竞争，分类效果更好；Fast R-CNN去掉了SVM这一步，所有的特征都暂存在显存中，就不需要额外的磁盘空间。 </p>
<h4 id="2-训练过程"><a href="#2-训练过程" class="headerlink" title="2.训练过程"></a>2.<strong>训练过程</strong></h4><ol>
<li><p><strong>有监督预训练</strong></p>
<table>
<thead>
<tr>
<th>样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>ILSVRC 20XX</td>
</tr>
<tr>
<td>负样本</td>
<td>ILSVRC 20XX</td>
</tr>
</tbody></table>
<p>ILSVRC 20XX样本只有类别标签，有1000种物体；<br>文中采用AlexNet【S for small】、VGG_CNN_M_1024【M for medium】、VGG-16【L for large】这三种网络分别进行训练测试，下面仅以VGG-16举例。</p>
</li>
<li><p><strong>特定样本下的微调</strong></p>
<table>
<thead>
<tr>
<th>样本</th>
<th>比例</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>25%</td>
<td>与某类Ground Truth相交IoU∈[0.5,1]的候选框</td>
</tr>
<tr>
<td>负样本</td>
<td>75%</td>
<td>与20类Ground Truth相交IoU中最大值∈[0.1,0.5）的候选框</td>
</tr>
</tbody></table>
<p>PASCAL VOC数据集中既有物体类别标签，也有物体位置标签，有20种物体；<br>正样本仅表示前景，负样本仅表示背景；<br>回归操作仅针对正样本进行；<br>该阶段训练集扩充方式：50%概率水平翻转；</p>
<p>微调前，需要对有监督预训练后的模型进行3步转化：<br>①RoI池化层取代有监督预训练后的VGG-16网络最后一层池化层；<br>②两个并行层取代上述VGG-16网络的最后一层全连接层和softmax层，并行层之一是新全连接层1+原softmax层1000个分类输出修改为21个分类输出【20种类+背景】，并行层之二是新全连接层2+候选区域窗口回归层，如下图所示；<br>③上述网络由原来单输入：一系列图像修改为双输入：一系列图像和这些图像中的一系列候选区域；</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsr7rdzezzj30dr05atbg.jpg" alt="这里写图片描述"></p>
<p>SGD超参数选择：<br>除了修改增加的层，原有的层参数已经通过预训练方式初始化；<br>用于分类的全连接层以均值为0、标准差为0.01的高斯分布初始化，用于回归的全连接层以均值为0、标准差为0.001的高斯分布初始化，偏置都初始化为0；<br>针对PASCAL VOC 2007和2012训练集，前30k次迭代全局学习率为0.001，每层权重学习率为1倍，偏置学习率为2倍，后10k次迭代全局学习率更新为0.0001；<br>动量设置为0.9，权重衰减设置为0.0005。</p>
<p><strong>。解释分析</strong></p>
<ol>
<li><p>Fast R-CNN如何采样进行SGD训练，和R-CNN、SPPnet中SGD采样方式有什么区别和优势？<br>R-CNN和SPPnet中采用RoI-centric sampling：从所有图片的所有候选区域中均匀取样，这样每个SGD的mini-batch中包含了不同图像的样本，不同图像之间不能共享卷积计算和内存，运算开销大；<br>Fast R-CNN中采用image-centric sampling： mini-batch采用层次采样，即先对图像采样【N个】，再在采样到的图像中对候选区域采样【每个图像中采样R/N个，一个mini-batch共计R个候选区域样本】，同一图像的候选区域卷积共享计算和内存，降低了运算开销；<br>image-centric sampling方式采样的候选区域来自于同一图像，相互之间存在相关性，可能会减慢训练收敛的速度，但是作者在实际实验中并没有出现这样的担忧，反而使用N=2，R=128的image-centric sampling方式比R-CNN收敛更快。</p>
</li>
<li><p>训练数据越多效果越好吗？</p>
<table>
<thead>
<tr>
<th>实验</th>
<th>训练集</th>
<th>测试集</th>
<th>mAP</th>
</tr>
</thead>
<tbody><tr>
<td>实验1</td>
<td>VOC 2007训练集</td>
<td>VOC 2007测试集</td>
<td>66.9%</td>
</tr>
<tr>
<td>实验1</td>
<td>VOC 2007+VOC 2012训练集</td>
<td>VOC 2007测试集</td>
<td>70.0%</td>
</tr>
<tr>
<td>实验2</td>
<td>VOC 2012训练集</td>
<td>VOC 2010测试集</td>
<td>66.1%</td>
</tr>
<tr>
<td>实验2</td>
<td>VOC 2007+VOC 2012训练集+VOC2007测试集</td>
<td>VOC 2010测试集</td>
<td>68.8%</td>
</tr>
<tr>
<td>实验3</td>
<td>VOC 2012训练集</td>
<td>VOC 2012测试集</td>
<td>65.7%</td>
</tr>
<tr>
<td>实验3</td>
<td>VOC 2007+VOC 2012训练集+VOC2007测试集</td>
<td>VOC 2012测试集</td>
<td>68.4%</td>
</tr>
</tbody></table>
<p>文中分别在VOC 2007、VOC 2010、VOC 2012测试集上测试，发现训练数据越多，效果确实更好。这里微调时采用100k次迭代，每40k次迭代学习率都缩小10倍。</p>
</li>
<li><p>哪些层参数需要被微调？<br>SPPnet论文中采用ZFnet【AlexNet的改进版】这样的小网络，其在微调阶段仅对全连接层进行微调，就足以保证较高的精度，作者文中采用VGG-16【L for large】网路，若仅仅只对全连接层进行微调，mAP会从66.9%降低到61.4%， 所以文中也需要对RoI池化层之前的卷积层进行微调；</p>
<p>那么问题来了？向前微调多少层呢？所有的卷积层都需要微调吗？<br>作者经过实验发现仅需要对conv3_1及以后卷积层【即9-13号卷积层】进行微调，才使得mAP、训练速度、训练时GPU占用显存三个量得以权衡；<br>作者说明所有AlexNet【S for small】、VGG_CNN_M_1024【M for medium】的实验结果都是从conv2往后微调，所有VGG-16【L for large】的实验结果都是从conv3_1往后微调。</p>
</li>
<li><p>Fast R-CNN如何进行多任务训练？多任务训练有效果吗？<br>Fast R-CNN网络分类损失和回归损失如下图所示【仅针对一个RoI即一类物体说明】，黄色框表示训练数据，绿色框表示输入目标：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fsr90mwyoyj30nr04taaw.jpg" alt="这里写图片描述"></p>
<p>-cls_score层用于分类，输出K+1维数组p，表示属于K类物体和背景的概率；<br>-bbox_predict层用于调整候选区域位置，输出4*K维数组，也就是说对于每个类别都会训练一个单独的回归器； </p>
<p>Fast RCNN有两个输出层：</p>
<p>一个对每个RoI输出离散概率分布：<img src="https://ws4.sinaimg.cn/large/006tKfTcly1fsra9v9i4aj304200njr6.jpg" alt="img"></p>
<p>一个输出bounding box回归的位移：<img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsra9z0kizj304s00t743.jpg" alt="img"></p>
<p>k表示类别的索引，前两个参数是指相对于object proposal尺度不变的平移，后两个参数是指对数空间中相对于object proposal的高与宽。</p>
<p>每个训练的RoI都被标记了ground-truth类别 u 以及ground-truth边界框回归 v 。在每个标记好的RoI上用multi-task loss 函数来级联的训练分类和bbox边界框回归：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fsraa26uz7j30bh01474a.jpg" alt="img"></p>
<p>约定u=0为背景分类，那么[u≥1] 函数表示背景候选区域即负样本不参与回归损失，不需要对候选区域进行回归操作； </p>
<p>第一项是对于 u 类的分类损失（log loss for true class u）。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fsraa6zeypj304u00vmwz.jpg" alt="img"></p>
<p>对于分类loss，是一个N+1路的softmax输出，损失函数是交叉熵函数， 其中的N是类别个数，1是背景。</p>
<p>第二项是回归损失，是在 u 类的真正边界框回归目标的元组 v 上定义的，是一个 4xN 路输出的regressor，也就是说对于每个类别都会训练一个单独的regressor，评估回归损失代价就是比较真实分类 u 对应的预测平移缩放参数和真实平移缩放参数的差距：</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fsraab1qfhj30a201fq2y.jpg" alt="img"></p>
<p>其中，</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsraaesfv6j305000qmwy.jpg" alt="img"></p>
<p>是真实平移缩放参数</p>
<p>对于 u 重新预测bbox回归平移缩放参数：<img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsraaj0bcnj304q00o3yb.jpg" alt="img"></p>
<p>这里的损失不是L2损失函数，而是smooth L1损失函数，对于离群点不敏感，因为有L2损失的训练可能需要仔细调整学习率，以防止爆炸梯度（控制梯度的量级使得训练时不容易跑飞）。</p>
<p>公式如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fsraamebbaj309e0240ss.jpg" alt="img"></p>
<p>超参数 λ 是用来控制两个损失函数的平衡的。作者对回归目标$v_i$进行归一化使其具有零均值及单位权方（zero mean and unit variance）。所有的函数都设置超参数 λ = 1。</p>
<p>smooth L1损失函数曲线如下图所示，相比于L2损失函数，其对离群点、异常值不敏感，可控制梯度的量级使训练时不容易跑飞</p>
</li>
</ol>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fssc2v8o77j30br08ujro.jpg" alt="这里写图片描述"></p>
<p>那多任务训练有效果吗？<br>首先不看多任务训练效果，至少比起R-CNN其训练方便、简洁。多任务训练考虑各任务间共享卷积层的相互影响，是有潜在可能提高检测效果的；<br>文中通过实验发现AlexNet【S for small】、VGG_CNN_M_1024【M for medium】、VGG-16【L for large】三种网络采用多任务训练比不采用mAP提高了0.8%~1.1%【测试时不采用Bounding-box regression】。 </p>
<ol start="5">
<li>RoI池化层如何进行反向求导训练？<br>首先看普通max pooling层如何求导，设xi为输入层节点，yi为输出层节点，那么损失函数L对输入层节点xi的梯度为        $\frac{∂<em>L}{∂</em>{x_i}}=\begin{cases}0,&amp;δ(i,j)=false\ \frac{∂<em>L}{∂</em>{yj}}&amp;{δ(i,j)=true }\end{cases}$</li>
</ol>
</li>
</ol>
<p>   <img src="https://ws3.sinaimg.cn/large/006tKfTcly1fssc2ip7anj30d008c3yo.jpg" alt="这里写图片描述"></p>
<p>   其中判决函数$δ(i,j)$表示输入i节点是否被输出j节点选为最大值输出（需要记录max_id）。不被选中$δ(i,j)=false$有两种可能：$x_i$不在$y_i$范围内，或者$x_i$不是最大值。若选中$δ(i,j)=true$ 则由链式规则可知损失函数L相对$x_i$的梯度等于损失函数L相对$y_i$的梯度×（$y_i$对$x_i$的梯度-&gt;恒等于1），故可得上述所示公式；</p>
<p>   对于RoI max pooling层，设$x_i$为输入层的节点，$y_{ri}$ 为第r个候选区域的第j个输出节点，一个输入节点可能和多个输出节点相关连，因为这些feature map 都是由RoI投射过来的，如下图所示，输入节点7和两个候选区域输出节点相关连（怎么判断它和哪个候选区域相连？）；</p>
<p>   <img src="https://ws1.sinaimg.cn/large/0069RVTdly1fu0chcjjmlj30ww0gydiv.jpg" alt="屏幕快照 2018-08-06 下午10.20.08"></p>
<p>   该输入节点7的反向传播如下图所示。对于不同候选区域，节点7都存在梯度，所以反向传播中损失函数L对输入层节点xi的梯度为损失函数L对各个有可能的候选区域r【xi被候选区域r的第j个输出节点选为最大值 】输出yri梯度的累加，具体如下公式所示：</p>
<p>   $\frac{∂<em>L}{∂</em>{xi}}=∑<em>r∑_j[i=i^∗(r,j)]\frac{∂L}{∂y</em>{rj}}$</p>
<p>​    </p>
<p>   $[i=i∗(r,j)]=\begin{cases}1,&amp;i=i∗(r,j)≥1\0,&amp;otherwise\end{cases} $</p>
<p>   判决函数$[i=i^*(r,j)]$表示i节点是否被候选区域r的第j个输出节点选为最大值输出，若是，则由链式规则可知损失函数L相对xi的梯度等于损失函数L相对$y_{rj}$的梯度x（$y_{rj}$对$x_i$的梯度-&gt;恒等于1)，上图已然解释该输入节点可能会和不同的yrj有关系，故损失函数L相对xi的梯度为求和形式。</p>
<h4 id="4-结果怎么样"><a href="#4-结果怎么样" class="headerlink" title="4.结果怎么样"></a>4.<strong>结果怎么样</strong></h4><ol>
<li>PASCAL VOC 2007训练集上，使用VGG-16【L for large】网络Fast R-CNN训练时间为9.5h，同等条件下R-CNN需要84h，快8.8倍；</li>
<li>PASCAL VOC 2007测试集上，使用VGG-16【L for large】网络不采用SVD Fast R-CNN测试时间为0.32s/image【不包括候选区域提取时间】，同等条件下R-CNN需要47.0s/image，快146倍；采用SVD测试时间为0.22s/image【不包括候选区域提取时间】，快213倍；</li>
<li>PASCAL VOC 2007测试集上，使用VGG-16【L for large】网络不采用SVD Fast R-CNN mAP为66.9%，同等条件下R-CNN mAP为66.0%；Fast R-CNN采用SVD mAP为66.6%。</li>
</ol>
<h4 id="5-还存在什么问题"><a href="#5-还存在什么问题" class="headerlink" title="5.还存在什么问题"></a>5.<strong>还存在什么问题</strong></h4><ol>
<li><p>Fast R-CNN中采用selective search算法提取候选区域，而目标检测大多数时间都消耗在这里【selective search算法候选区域提取需要2~3s，而提特征分类只需要0.32s】，这无法满足实时应用需求，而且Fast R-CNN并没有实现真正意义上的端到端训练模式【候选区域是使用selective search算法先提取出来的】；</p>
<p>那有没有可能使用CNN直接产生候选区域并对其分类呢？Faster R-CNN框架就是符合这样需求的目标检测框架.</p>
</li>
</ol>
<h3 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster-RCNN"></a>Faster-RCNN</h3><h4 id="1-创新点（解决Fast-RCNN存在的问题）"><a href="#1-创新点（解决Fast-RCNN存在的问题）" class="headerlink" title="1.创新点（解决Fast-RCNN存在的问题）"></a>1.创新点（解决Fast-RCNN存在的问题）</h4><p>  1.设计Region Proposal Networks[RPN],利用CNN卷积操作后的特征图生成region proposals,代替Selective Search,EdgeBoxes等方法，速度提升明显。</p>
<p>  2.训练Region Proposal Networks与检测网络【Fast R-CNN】共享卷积层，大幅提高网络检测速度。</p>
<h4 id="2-Faster-RCNN网络结构图"><a href="#2-Faster-RCNN网络结构图" class="headerlink" title="2.Faster-RCNN网络结构图"></a>2.Faster-RCNN网络结构图</h4><p>  Faster-RCNN网络架构图如下，可以简单的看做RPN网络+Fast R-CNN网络</p>
<p>  <img src="https://ws1.sinaimg.cn/large/006tKfTcly1fsunxeb7qnj30lz0aimxs.jpg" alt="Faster-RCNN网络结构图"></p>
<p>  1.首先向CNN网络【ZF或VGG-16】输入任意大小图片；</p>
<p>  2.经过CNN网络前向传播至最后共享的卷积层，一方面得到供RPN网络使用的特征图，另一方面继续前向传播至特有卷积层，产生更高维的特征图；</p>
<p>  3.供RPN网络输入的特征图经过RPN网络得到区域建议和区域得分，并对区域得分采用非极大值抑制【阈值为0.7】，输出其Top-N[文中为300]得分的区域建议给ROI池化层。</p>
<p>  4.第2步得到的高维特征图和第三步输出的区域建议同时输入ROI池化层，提取对应区域建议的特征。</p>
<h4 id="3-RPN网络是怎么样的？"><a href="#3-RPN网络是怎么样的？" class="headerlink" title="3.RPN网络是怎么样的？"></a>3.RPN网络是怎么样的？</h4><p>  ​    单个RPN网络结构如下图：   <img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsuolz05rej30je0bkt95.jpg" alt="这里写图片描述"></p>
<p>  1.卷积层得到的特征是，conv feature map的维度是13x13x256的；</p>
<p>  2.作者在文章中指出，sliding window的大小是3x3的，那么如何得到一个256-d的向量呢？</p>
<p>  这个就很简单了，我们只需要一个3x3x256x256的卷积核，就可以将每一个3x3的的sliding window卷积成一个256维的向量；</p>
<p>  需要注意的是，作者文中所画的示意图，仅仅是针对一个sliding window 的，在实际实现中，我们有很多个sliding window，所以得到的并不是一个一维的256d向量，实际上还是一个三维的矩阵数据结构(HXWX256)；可能写成for循环做sliding window 大家会比较清楚，当用矩阵云散的时候会稍微绕一些</p>
<p>  3.步骤2中的低维特征向量，输入两个并行连接的卷积层2：reg窗口回归【位置精修】和 cls窗口分类，分别用于回归区域建议产生的bounding-box，和对区域建议是佛为前景或背景打分，这里由于每个滑窗位置产生K个区域建议，所以reg层有4k个输出来编码，cls层有2k个得分估计K个区域建议为前景或者背景的概率。</p>
<p>  4.论文中为每个滑窗预测9个可能的参考窗口(文中是anchors)，然后就是k=9,所以cls layer就是18个输出节点了，那么256d和cls layer之间使用一个1x1x256x18的卷积核，就可以得到cls layer，其实这里就是全连接层；</p>
<p>  5.reg layer 也是一样了，reg layer的输出是36个，所以对应的卷积核是1x1x256x36，这样就得到 reg layer的输出了；</p>
<p>  6.然后 cls layer和reg layer 后面都会接到自己的损失函数上，给出损失函数的值，同时会根据求导的结果，给出反向传播的数据。</p>
<h4 id="4-Anchors是什么？有什么用？"><a href="#4-Anchors是什么？有什么用？" class="headerlink" title="4.Anchors是什么？有什么用？"></a>4.Anchors是什么？有什么用？</h4><p>  1.Anchors是一组大小固定的参考窗口：三种尺度{128^2, 256^2, 512^2}x三种长宽比{1：1，1：2，2：1}，如下图所示，表示RPN网络中对特征图滑窗时每个滑窗位置所对应的原图区域中9种可能的大小，相当于模板，对任意图像任意滑窗位置都是这9种模板。继而根据图像大小计算滑窗中心点对应原图区域的中心点，通过中心点和size就可以得到滑窗位置和原图位置的映射关系，由此原图位置并根据与Ground Truth的IOU大小，贴上标签，让RPN学习该Anchors是否有物体即可。</p>
<p>  <img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsvibez7uxj308t080q3d.jpg" alt="这里写图片描述"></p>
<p>  作者在文中表示采用Anchors这种方法具有平移不变性，就是说在图像中平移了物体，窗口建议也会跟着平移，（应该是学习的结果）</p>
<p>  2.Anchors为什么考虑以上三种尺度和长宽比？</p>
<p>  文中对Anchors的尺度以及长宽比选取进行了实验，如下图所示：</p>
<p>  <img src="https://ws4.sinaimg.cn/large/006tKfTcly1fsvj0gbpefj30bh03s74e.jpg" alt="这里写图片描述"></p>
<p>  实验在VGG-16模型下，采用PASCAL VOC2007训练集和PASCAL VOV2007 测试集得到。相比于只采用单一尺度和长宽比，单尺度多长宽比和多尺度单长宽比都能提升mAP,表明多size的anchors可以提高mAP。</p>
<p>  3.如何处理多尺度多长宽比的问题？即如何使24x24和1080x720的车辆同时在一个训练好的网络中都能正确识别？</p>
<p>  文中展示了两种解决多尺度长宽比问题：一种是使用图像金字塔，对伸缩到不同size的输入图像进行提取，虽然有效但是费时；</p>
<p>  另一种是使用滤波器金字塔或者滑动窗口金字塔，对输入图像采用不同size的滤波器分别进行卷积操作，这两种方式都需要枚举图像或者滤波器size.</p>
<p>  作者提出了一种叫做Anchors金字塔的方法来解决多尺度多长宽比问题，在RPN网络中对特征图滑窗时，对滑窗位置中心进行多尺度多长宽比采样，并对多尺度多长宽比的anchor boxes区域进行回归和分类，利用Anchors金字塔就仅仅依赖于单一尺度的图像和特征图和单一大小的卷积核，就可以解决多尺度长宽比问题，这种对推荐区域采样的模型，不管是速度还是准确率都能取得很好的性能。</p>
<p>  4.RPN网络的结构</p>
<p>  <img src="https://ws3.sinaimg.cn/large/0069RVTdly1fu1hwhs72fj30ym0qyal9.jpg" alt="屏幕快照 2018-08-07 下午10.13.23"></p>
<p>  data:         1*3*600<em>1000<br>  gt_boxes: N</em>5,                   N为groundtruth box的个数，每一行为(x1, y1, x2, y2, cls) ，而且这里的gt_box是经过缩放的。<br>  im_info： 1*3                   （h,w,scale） </p>
<p>  rpn_cls_score是cls层输出的18通道，shape可以看成是1*18*H*W.  （）</p>
<p>  输出为4个量：rpn_labels 、rpn_bbox_targets（回归目标）、rpn_bbox_inside_weights（内权重）、rpn_bbox_outside_weights（外权重）。</p>
<p>  通俗地来讲，这一层产生了具体的anchor坐标，并与groundtruth box进行了重叠度计算，输出了label与回归目标。(生成每一个点9个anchorbox的类别前景还是背景，以及前景需要回归的groundtruth)</p>
<p>  rpn_labels ： (1, 1, 9 * height, width)</p>
<p>  rpn_bbox_targets（回归目标）： (1, 36，height, width)</p>
<p>  rpn_bbox_inside_weights（内权重）：(1, 36，height, width)(在计算损失的时候，用来平衡x,y,h,w的损失占比，不过在 程序中都设为1)</p>
<p>  rpn_bbox_outside_weights（外权重）：(1, 36，height, width)（在程序中都设为1/Nreg）</p>
<p>  回到stage1_rpn_train.pt，接下里我们就可以利用rpn_cls_score_reshape与rpn_labels计算SoftmaxWithLoss，输出rpn_cls_loss。</p>
<p>  而regression可以利用rpn_bbox_pred，rpn_bbox_targets，rpn_bbox_inside_weights，rpn_bbox_outside_weights计算SmoothL1Loss，输出rpn_loss_bbox。</p>
<p>​       </p>
<h4 id="5-训练过程"><a href="#5-训练过程" class="headerlink" title="5.训练过程"></a>5.训练过程</h4><ol>
<li><p><strong>RPN网络预训练</strong></p>
<table>
<thead>
<tr>
<th>样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>ILSVRC20XX</td>
</tr>
<tr>
<td>负样本</td>
<td>ILSVRC20XX</td>
</tr>
</tbody></table>
<p>样本中只有类别标签；<br>文中一带而过RPN网络被ImageNet网络【ZF或VGG-16】进行了有监督预训练，利用其训练好的网络参数初始化；<br>用标准差0.01均值为0的高斯分布对新增的层随机初始化。</p>
</li>
<li><p><strong>Fast R-CNN网络预训练</strong></p>
<table>
<thead>
<tr>
<th>样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>ILSVRC20XX</td>
</tr>
<tr>
<td>负样本</td>
<td>ILSVRC20XX</td>
</tr>
</tbody></table>
<p>样本中只有类别标签；</p>
<p>文中一带而过Fast-RCNN网络被imageNet网络【ZF或VGG-16】进行了有监督预训练，利用其训练好的网络 参数初始化。</p>
</li>
<li><p><strong>RPN网络微调训练</strong></p>
<table>
<thead>
<tr>
<th>RPN网络样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>与Ground Truth相交IoU最大的anchors【以防后一种方式下没有正样本】+与Ground Truth相交IoU&gt;0.7的anchors</td>
</tr>
<tr>
<td>负样本</td>
<td>与Ground Truth相交IoU&lt;0.3的anchors</td>
</tr>
</tbody></table>
<p> PASACAL VOC 数据集中既有物体类别标签，也有物体位置标签；</p>
<p>正样本仅表示前景，负样本仅表示背景；</p>
<p>正样本仅表示前景，负样本仅表示背景；</p>
<p>回归操作仅针对正样本进行；</p>
<p>训练时弃用所有超出图像边界的anchors,否则在训练过程中会产生较大难以处理的修正误差项，导致训练过程无法收敛；</p>
<p>对去掉超出边界的后的anchors集采用非极大抑制，最终一张图有300个anchors用于训练；</p>
<p>对于ZF网络微调所有层，对于VGG-16网络仅微调conv3_1conv3_1以上的层，以便节省内存。</p>
<p><strong>SGDmini-batch采样方式</strong>：同Fast-RCNN网络，采取“image-centric”方式采样，即采用层次采样，先对图像采样，再对anchors取样，同一图像的anchors共享计算和内存。每个mini-batch包含从一张图像中随机提取的256个anchors，正负样本比例为1：1，如果一张图片 不够128个样本，拿负样本补齐。</p>
<p><strong>训练超参数选择：</strong> 在PASCAL VOC数据集上前60k次迭代的学习效率为0.001，后20K次迭代的学习率为0.0001；动量设置为0.9，权重衰减设置为0.0005.</p>
<p>一张图片多任务目标函数【分类+回归损失】具体如下：</p>
<p>$L({p_i},{t_i})=\frac{1}{N_{cls}}∑<em>iLcls(p_i,p_i^∗)+λ\frac{1}{N</em>{reg}}∑<em>ip^∗_iL</em>{reg}(t_i,t^∗_i)$ </p>
<p>当anchor为正样本时，$p_i^<em>=1$,当anchor为负样本时$p_i^</em>=0$,由此可以看出，回归损失仅在anchor为正样本时才被激活。</p>
<p>$t_i$表示正样本anchor到预测区域的4个平移缩放参数【以anchor为基准的变换】；$t_i^*$表示正样本anchor到groundtruth的4个平移缩放参数。【以anchor为基准的变换】；</p>
<p>分类损失函数</p>
<p>$L_{cls}$是一个二值【是物体或者不是物体】分类器，</p>
<p>$L_{cls}(p_i,p_i^<em>)=-log[p_i^*p_i+(1-p_i^</em>)(1-p_i)]$;</p>
<p>回归损失函数$L_reg(t_i,t_i^<em>)=R(t_i-t_i^</em>)$[两种变换之差越小越好]，R函数定义如下：</p>
</li>
</ol>
<p>​         </p>
<p>​     <img src="https://ws1.sinaimg.cn/large/006tKfTcly1fsraamebbaj309e0240ss.jpg" alt="img"></p>
<p>​         </p>
<p>​     默认值，$\lambda=10$ [论文中实验表明，$\lambda$从1变化到100对mAp的影响不超过100]</p>
<p>​     $N_{cls}$和$N_{reg}$分别用来标准化分类损失项$L_{cls}$ 和回归损失项$L_{reg}$，默认用mini-batch size=256设置$N_{cls}$，用anchor位置数目~2400初始化$N_{reg}$，文中也说明标准化操作并不是必须的，可以简化省略。</p>
<p>​         </p>
<p>​     4.<strong>Fast R-CNN网络微调训练</strong></p>
<table>
<thead>
<tr>
<th>Fast R-CNN网络样本</th>
<th>来源</th>
</tr>
</thead>
<tbody><tr>
<td>正样本</td>
<td>Ground Truth +与Ground Truth相交IoU&gt;阈值的区域建议</td>
</tr>
<tr>
<td>负样本</td>
<td>与Ground Truth相交IoU&lt;阈值的区域建议</td>
</tr>
</tbody></table>
<p>​     PASCAL VOC 数据集中既有物体类别标签，也有物体位置标签；<br>​     正样本表示每类物品的Ground Truth以及与Ground Truth重叠度超过某一阈值的区域建议，负样本表示同Ground Truth重叠度小于某一阈值的区域建议；<br>​     回归操作仅针对正样本进行。</p>
<p>​     5.<strong>RPN网络、Fast R-CNN网络联合训练</strong> </p>
<p>​     训练网络结构示意图如下所示：</p>
<p>​     <img src="https://ws4.sinaimg.cn/large/006tNc79ly1fswn24jfk1j30n309a74r.jpg" alt="这里写图片描述">如上图所示，RPN网络、Fast R-CNN网络联合训练是为了让两个网络共享卷积层，降低计算量。 </p>
<p>​     文中通过4步训练算法，交替优化学习至共享特征：<br>​     ① 进行上面RPN网络预训练，和以区域建议为目的的RPN网络end-to-end微调训练；<br>​     ② 进行上面Fast R-CNN网络预训练，用第①步中得到的区域建议进行以检测为目的的Fast R-CNN网络end-to-end微调训练【此时无共享卷积层】；<br>​     ③ 使用第②步中微调后的Fast R-CNN网络重新初始化RPN网络，固定共享卷积层【即设置学习率为0，不更新】，仅微调RPN网络独有的层【此时共享卷积层】；<br>​     ④ 固定第③步中共享卷积层，利用第③步中得到的区域建议，仅微调Fast R-CNN独有的层，至此形成统一网络如上图所示</p>
<p>​     6.解释分析</p>
<ol>
<li><p>RPN网络中bounding-box回归怎么理解？同Fast R-CNN中的bounding-box回归相比有什么区别？<br>对于bounding-box回归，采用以下公式：</p>
<p> ‘’ $t_x=(x−x_a)/w_a$         $ t_y=(y−y_a)/h_a$​            </p>
</li>
</ol>
<p>​        $t_w=log(w/w_a)$           $t_h=log(h/h_a)$</p>
<p>​            </p>
<p>​        $t^∗_x=(x^∗−x_a)/w_a $      $t^∗_y=(y^∗−y_a)/h_a$</p>
<p>​            </p>
<p>​        $t^∗_w=log(w^∗/w_a)$        $t^∗_h=log(h∗/h_a)$</p>
<p>​            </p>
<p>​        其中，x，y，w，h表示窗口中心坐标和窗口的宽度和高度，变量x，xa和x∗分别表示预测窗口、anchor窗口和Ground Truth的坐标【y，w，h同理】，因此这可以被认为是一个从anchor窗口到附近Ground Truth的bounding-box 回归；</p>
<p>​        RPN网络中bounding-box回归的实质其实就是计算出预测窗口。这里以anchor窗口为基准，计算Ground Truth对其的平移缩放变化参数，以及预测窗口【可能第一次迭代就是anchor】对其的平移缩放参数，因为是以anchor窗口为基准，所以只要使这两组参数越接近，以此构建目标函数求最小值，那预测窗口就越接近Ground Truth，达到回归的目的；</p>
<p>​        文中提到， Fast R-CNN中基于RoI的bounding-box回归所输入的特征是在特征图上对任意size的RoIs进行Pool操作提取的，所有size RoI共享回归参数，而在Faster R-CNN中，用来bounding-box回归所输入的特征是在特征图上相同的空间size【3×3】上提取的，为了解决不同尺度变化的问题，同时训练和学习了k个不同的回归器，依次对应为上述9种anchors，这k个回归量并不分享权重。因此尽管特征提取上空间是固定的【3×3】，但由于anchors的设计，仍能够预测不同size的窗口。</p>
<p>​     2.文中提到了三种共享特征网络的训练方式？<br>​     ① 交替训练<br>​     训练RPN，得到的区域建议来训练Fast R-CNN网络进行微调；此时网络用来初始化RPN网络，迭代此过程【文中所有实验采用】；</p>
<p>​     ② 近似联合训练<br>​     如上图所示，合并两个网络进行训练，前向计算产生的区域建议被固定以训练Fast R-CNN；反向计算到共享卷积层时RPN网络损失和Fast R-CNN网络损失叠加进行优化，但此时把区域建议【Fast R-CNN输入，需要计算梯度并更新】当成固定值看待，忽视了Fast R-CNN一个输入：区域建议的导数，则无法更新训练，所以称之为近似联合训练。实验发现，这种方法得到和交替训练相近的结果，还能减少20%~25%的训练时间，公开的python代码中使用这种方法；</p>
<p>​     ③ 联合训练<br>​     需要RoI池化层对区域建议可微，需要RoI变形层实现，具体请参考这片paper：<a href="http://xueshu.baidu.com/s?wd=paperuri:%28afbf3253b7d82a976a553564844442fa%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http://arxiv.org/abs/1512.04412&ie=utf-8&sc_us=10457437465023674419" target="_blank" rel="noopener">Instance-aware Semantic Segmentation via Multi-task Network Cascades</a>。</p>
<p>​         </p>
<p>​     文中训练过程</p>
<p>​     第一步：用model初始化RPN网络，然后训练RPN，在训练后，model以及RPN的unique会被更新。 </p>
<p>​     第二步：用model初始化Fast-rcnn网络，注意这个model和第一步一样。然后使用训练过的RPN来计算proposal，再将proposal给予Fast-rcnn网络。接着训练Fast-rcnn。训练完以后，model以及Fast-rcnn的unique都会被更新。 说明：第一和第二步，用同样的model初始化RPN网络和Fast-rcnn网络，然后各自独立地进行训练，所以训练后，各自对model的更新一定是不一样的（论文中的different ways），因此就意味着model是不共享的（论文中的dont share convolution layers）。   </p>
<p>​     第三步：使用第二步训练完成的model来初始化RPN网络，第二次训练RPN网络。但是这次要把model锁定，训练过程中，model始终保持不变，而RPN的unique会被改变。 说明：因为这一次的训练过程中，model始终保持和上一步Fast-rcnn中model一致，所以就称之为着共享。   </p>
<p>​     第四步：仍然保持第三步的model不变，初始化Fast-rcnn，第二次训练Fast-rcnn网络。其实就是对其unique进行finetune，训练完毕，得到一个文中所说的unified network。 </p>
<p>​     3.图像scale细节问题</p>
<p>​     网上关于Faster R-CNN中三种尺度这么解释：  原始尺度：原始输入的大小，不受任何限制，不影响性能；  归一化尺度：输入特征提取网络的大小，在测试时设置，源码中opts.test_scale=600。anchor在这个尺度上设定，这个参数和anchor的相对大小决定了想要检测的目标范围；  网络输入尺度：输入特征检测网络的大小，在训练时设置，源码中为224×224。 </p>
<p>​     4.理清文中anchors的数目。<br>​     文中提到对于1000×600的一张图像，大约有20000(~60×40×9)个anchors，忽略超出边界的anchors剩下6000个anchors，利用非极大值抑制去掉重叠区域，剩2000个区域建议用于训练；<br>​     测试时在2000个区域建议中选择Top-N【文中为300】个区域建议用于Fast R-CNN检测。</p>
<p>​     <strong>&amp;结果怎么样</strong></p>
<ol>
<li><p>PASCAL VOC实验【使用ZF网络】</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>数目</th>
</tr>
</thead>
<tbody><tr>
<td>目标类别</td>
<td>20</td>
</tr>
<tr>
<td>PASCAL VOC 2007训练集</td>
<td>5k</td>
</tr>
<tr>
<td>PASCAL VOC 2007测试集</td>
<td>5k</td>
</tr>
</tbody></table>
<p>a.第1组实验<br>目的：验证RPN方法的有效性；<br>做法：ZF检测网络训练和测试时分别使用Selective Search、EdgeBoxes和RPN+ZF【共享】方法，Selective Search、EdgeBoxes测试时使用2000窗口建议，RPN+ZF测试时采用300窗口建议；<br>结果：RPN+ZF方法获得59.9%的mAP，由于卷积层共享并且只有300个候选窗口，RPN+ZF方法检测速度更快；</p>
<p>b.第2组实验<br>目的：验证RPN和ZF检测网络共享卷积层的影响；<br>做法：在之前所述4步训练算法进行到第2步后停止；<br>结果：未实现卷积层共享的RPN+ZF的方法获得58.7%的mAP，这由于4步训练算法的第3步使用了微调后检测器特征来微调RPN网络，使得建议窗口质量得到提高；</p>
<p>c.第3组实验<br>目的：使用不同RPN候选窗数目下，评估其对检测网络mAP的影响；<br>做法：使用Selective Search方法训练检测网络ZF并固定不变【RPN与ZF没有共享卷积层】，测试时采用不同RPN候选窗数目进行；<br>结果：测试时300候选窗RPN获得56.8%的mAP，这是由于训练和测试的区域建议方法不一致造成；使用Top-100窗口建议仍然有55.1%的mAP，说明Top-100结果比较准确；未使用非极大值抑制的6000个区域建议全部使用进行检测获得55.2%的mAP，说明非极大值抑制并未损坏精度，反而可能减少了误报；</p>
<p>d.第4组实验<br>目的：验证RPN网络cls窗口分类层影响；<br>做法：使用Selective Search方法训练检测网络ZF并固定不变【RPN与ZF没有共享卷积层】，移除RPN网络中cls窗口分类层【缺少分数就没有了非极大值抑制和Top排名】，从未评分的窗口建议中随机采用N个 ；<br>结果：N=1000时，mAP为55.8%影响不大，但N=100时mAP为44.6%，说明cls窗口分类层的评分准确度高，影响检测结果精度；</p>
<p>e.第5组实验<br>目的：验证RPN网络reg窗口回归层影响；<br>做法：使用Selective Search方法训练检测网络ZF并固定不变【RPN与ZF没有共享卷积层】，移除RPN网络reg窗口回归层【候选区域直接变成没有回归的anchor boxes】；<br>结果：选择Top-300进行实验，mAP掉到了52.1%，说明窗口回归提高了区域建议的质量，虽然说anchor boxes能应对不同尺度和宽高比，但是对于精确检测远远不够；</p>
<p>f.第6组实验<br>目的：验证优质量网络对RPN产生区域建议的影响；<br>做法：使用Selective Search方法训练检测网络ZF并固定不变【RPN与ZF没有共享卷积层】，采用VGG-16网络训练RPN提供候选区域；<br>结果：与第3组实验测试时300候选窗RPN获得56.8%的mAP相比，采用VGG-16训练RPN使得mAP达到59.2%，表明VGG-16+RPN提供区域建议质量更高【不像死板板的Selective Search，RPN可以从更好的网络中获利进行变化】，因此RPN和检测网络同时采用VGG-16并共享卷积层会如何呢？结果见下。</p>
</li>
<li><p>RPN网络和检测网络同时采用VGG-16并共享卷积层，在PASCAL VOC 2007训练集上训练，测试集上获得69.9%的mAP；在联合数据集如PASCAL VOC 2007和2012训练集上训练RPN网络和检测网络，PASCAL VOC 2007测试集上mAP会更高。</p>
</li>
<li><p>对于检测速度而言，采用ZF模型，可以达到17fps；采用VGG-16模型，可以达到5fps，由于卷积共享，RPN网络仅仅花10ms计算额外的层，而且，由于仅仅选取Top-N【文中为300】进行检测，检测网络中的非极大值抑制、池化、全连接以及softmax层花费时间是极短的。</p>
</li>
<li><p>召回率分析。所谓召回率即区域建议网络找出的为真的窗口与Ground Truth的比值【IoU大于阈值即为真】，文中实验表明Selective Search、EdgeBoxes方法从Top-2000、Top-1000到Top-300的召回率下降明显，区域建议越少下降越明显，而RPN网络召回率下降很少，说明RPN网络Top-300区域建议已经同Ground Truth相差无己，目的性更明确。</p>
</li>
<li><p>MS COCO实验【使用VGG-16网络】</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>数目</th>
</tr>
</thead>
<tbody><tr>
<td>目标类别</td>
<td>80</td>
</tr>
<tr>
<td>Microsoft COCO训练集</td>
<td>80k</td>
</tr>
<tr>
<td>Microsoft COCO验证集</td>
<td>40k</td>
</tr>
<tr>
<td>Microsoft COCO测试集</td>
<td>20k</td>
</tr>
</tbody></table>
<p>采用8-GPU并行训练，则RPN有效mini-batch 为8张图，Fast R-CNN有效mini-batch为16张图；<br>RPN和Fast R-CNN以0.003【由0.001改为0.003，由于有效mini-batch被改变了】的学习率迭代240k次，以0.0003的学习率迭代80k次；<br>对于anchors，在三种尺度三种长宽比基础上增加了64^2的尺度，这是为了处理Microsoft COCO数据集上的小目标【新数据集上不直接套用这一点值得学习】；<br>增加定义负样本IoU，重叠阈值由[0.1，0.5) 到[0，0.5)，这能提升COOC数据集上mAP；<br>使用COCO训练集训练，COCO测试集上获得42.1%的mAP @0.5 和21.5%的mAP @[.5，.95]。</p>
<p>6.与VGG-16相比，利用ResNet-101网络，在COCO验证集上mAP从41.5%/21.2%(@0.5/@[.5，.95])变化到48.4%/27.2%，归功于RPN网络可以从更好的特征提取网络中学到更好的区域建议。</p>
<p>7.由于Microsoft COCO数据集种类包含PASCAL VOC数据集种类，文中在Microsoft COCO数据集上训练，在PASCAL VOC数据集上测试，验证大数据量下训练是否有助于提高mAP?<br>采用VGG-16模型，当仅仅利用Microsoft COCO数据集训练时，PASCAL VOC 2007测试集上mAP达到76.1%【泛化能力强，未过拟合】；当利用Microsoft COCO数据集训练的模型初始化，PASCAL VOC 2007+2012训练集进行微调，PASCAL VOC 2007测试集上mAP达到78.8%，此时每一个单体类别的AP较其它样本训练的都达到最高，而每张图测试时间仍然约为200ms。</p>
</li>
</ol>
<p>​          </p>
<p>​     ——</p>
<p>​          </p>
<p>​           </p>
<p>​          </p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>xy</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://yoursite.com/2019/07/14/目标检测算法/">http://yoursite.com/2019/07/14/目标检测算法/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY<strong>?</strong></strong></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2019/07/14/blogtest/">blogtest</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xy | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
