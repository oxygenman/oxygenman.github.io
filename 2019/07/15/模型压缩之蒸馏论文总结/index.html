<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-cn">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="我看的论文,">










<meta name="description" content="图像分类1.2015-NIPS-Distilling the Knowledge in a Neural Network 论文摘要：该论文是Hinton对蒸馏概念的诠释，但是第一个提出蒸馏的方法的不是他，而是2104年的另一篇论文 。主要思想是将多个模型的支持整合到一个小的模型中，达到模型压缩的目的。该文章还提出了一种新的模型组合方式，使用一个或多个全模型和多个专家模型进行组合，可以达">
<meta name="keywords" content="我看的论文">
<meta property="og:type" content="article">
<meta property="og:title" content="模型压缩之蒸馏论文总结">
<meta property="og:url" content="http://yoursite.com/2019/07/15/模型压缩之蒸馏论文总结/index.html">
<meta property="og:site_name" content="Young">
<meta property="og:description" content="图像分类1.2015-NIPS-Distilling the Knowledge in a Neural Network 论文摘要：该论文是Hinton对蒸馏概念的诠释，但是第一个提出蒸馏的方法的不是他，而是2104年的另一篇论文 。主要思想是将多个模型的支持整合到一个小的模型中，达到模型压缩的目的。该文章还提出了一种新的模型组合方式，使用一个或多个全模型和多个专家模型进行组合，可以达">
<meta property="og:locale" content="zh-cn">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5darvpwhdj31c00u0n5l.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g58nd6kaiqj31c00u0qf8.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5dasgsm7ij31c00u0qe3.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g58ndmdsuqj31c00u0120.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g58ne2telcj31c00u0agp.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5dasqnhz9j31c00u07bb.jpg">
<meta property="og:image" content="https://camo.githubusercontent.com/cfee36c2ed92f4515c856267d1815cb1632312a7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f353532393939372d336566303536356132313565333966382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57hn81kbwj31am0ig79m.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006tNc79ly1g57jpyppefj318g070aam.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57jurpzcqj31bq0h0jtv.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57k0b2rvtj31ba0bggms.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57ktjamrvj30sy0g2mzr.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57lki10knj31gs0u0tgk.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57lwhafgbj30tk0cs763.jpg">
<meta property="og:image" content="http://ww4.sinaimg.cn/large/006tNc79ly1g57ly168sdj30t60ck0uj.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51k7g3xa9j313o0l60tr.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53ren7gzuj31cm0lgdh1.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51mqkr1wzj30yo0kiaek.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51mrmkldaj30nc0fkwhj.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g549nghgotj31am0bygoy.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57s3augprj30y10jkjtz.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57sj9fiaej30tg0nogpq.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57slsxg8wj30ty0hwtba.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57mvel4z0j30q40m0wkr.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57qrao0jqj30u40i4t9x.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51hrfjvvaj312y0eoq7c.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51i305natj31cu0cqjv1.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yo475yaj30sc0ccq51.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yqax3mlj30u20n2tci.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53ytkx7q6j30te0isgp2.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5c68zwqb9j30io08k3zt.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yzm5xpwj30u00c0tam.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yzxrpbej30sm0gk76z.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g540ls258aj31820tsgrz.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5417b20v3j31180scn42.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g541jdkwhrj30v70u07bl-20220717190330356.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g541lu1m7dj310c0e076z.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g541o5xe3zj31040cu76i.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g543cgs196j30to0si44j.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5449r87amj31mo0q6wh0.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g544fo2zu5j31iq0u0qay.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g54zjxb288j31800d2dhw.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g54zkhl1dhj30nc0pmq64.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g555d2woo1j30j605474v.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/006tNc79ly1g555gehz77j30iu09m0u1.jpg">
<meta property="og:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5c44c953uj30j40ayabb.jpg">
<meta property="og:updated_time" content="2022-07-17T11:05:14.420Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="模型压缩之蒸馏论文总结">
<meta name="twitter:description" content="图像分类1.2015-NIPS-Distilling the Knowledge in a Neural Network 论文摘要：该论文是Hinton对蒸馏概念的诠释，但是第一个提出蒸馏的方法的不是他，而是2104年的另一篇论文 。主要思想是将多个模型的支持整合到一个小的模型中，达到模型压缩的目的。该文章还提出了一种新的模型组合方式，使用一个或多个全模型和多个专家模型进行组合，可以达">
<meta name="twitter:image" content="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5darvpwhdj31c00u0n5l.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/15/模型压缩之蒸馏论文总结/">





  <title>模型压缩之蒸馏论文总结 | Young</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-cn">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Young</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/15/模型压缩之蒸馏论文总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xy">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">模型压缩之蒸馏论文总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-15T18:39:43+08:00">
                2019-07-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5darvpwhdj31c00u0n5l.jpg" alt="屏幕快照 2019-07-26 下午3.57.46"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g58nd6kaiqj31c00u0qf8.jpg" alt="image-20190722152711483"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5dasgsm7ij31c00u0qe3.jpg" alt="屏幕快照 2019-07-26 下午3.57.56"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g58ndmdsuqj31c00u0120.jpg" alt="image-20190722152735168"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g58ne2telcj31c00u0agp.jpg" alt="image-20190722152803527"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5dasqnhz9j31c00u07bb.jpg" alt="屏幕快照 2019-07-26 下午3.58.04"></p>
<h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><h4 id="1-2015-NIPS-Distilling-the-Knowledge-in-a-Neural-Network"><a href="#1-2015-NIPS-Distilling-the-Knowledge-in-a-Neural-Network" class="headerlink" title="1.2015-NIPS-Distilling the Knowledge in a Neural Network"></a>1.2015-NIPS-<a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></h4><ul>
<li><p>论文摘要：该论文是Hinton对蒸馏概念的诠释，但是第一个提出蒸馏的方法的不是他，而是2104年的<a href="https://arxiv.org/abs/1312.6184" target="_blank" rel="noopener">另一篇论文</a> 。主要思想是将多个模型的支持整合到一个小的模型中，达到模型压缩的目的。该文章还提出了一种新的模型组合方式，使用一个或多个全模型和多个专家模型进行组合，可以达到并行快速训练的效果。</p>
</li>
<li><p>实验数据集：MNIST, acoustic model</p>
</li>
<li><p>论文思想：</p>
<p>很多昆虫都都有幼虫阶段，从而更易于从生长的环境中吸收能力和维生素，然后变成成虫，更易于日后的种群迁移和繁衍。人也是如此，积累和很多段的人生经验才成就了现在的你。在机器学习算法中，我门大部分也经历了差不多的过程：在训练阶段我们需要从数量大，且存在高度冗余的数据集中提取特征，但是这样的模型病不便于应用，应用的时候需要进化的，提纯的模型。所以说蒸馏就是把重要的东西留下来。使用蒸馏的方法将大模型中的重要知识迁移到小模型中。</p>
<p>那么怎么蒸馏？蒸馏要留下的是大网络学习到的东西，大模型学到的是概率分布，老师要把学到的东西交给学生，使用大网络的输出信息，我们要尽可能的去学习大模型学到的东西。</p>
<p>怎么更好的蒸馏？Hinton认为正常的模型学习到的就是在正确类别上的最大概率，但是不正确的分类上也会得到一些概率。尽管有时候这些概率很小，但是对这些非正确类别的概率也包含了模型泛化的方向信息，包含了特征分布信息。使用这些信息更利于模型的泛化。要想使小模型获得泛化的能力，就要用大模型产生的类别概率，作为小模型的“soft targets”去训练小模型。比如对于MNIST数据集的训练来说，正确类别的概率往往比错误类比的概率大的多，比如在某次识别中，数字7识别为2的概率为$10^{-6}$ ,识别为3的概率为$10^{-9}$ ,这个信息是很有用它表明了2和3很像，但是它对交叉熵损失函数的影响确非常小。 不同与第一篇论文中使用最后一层logits进行训练，Hinton使用softmax输出的概率进行训练，并加入了一个温度参数，使“soft target”更加soft。</p>
<p>如何训练小模型？在训练阶段，可以使用大模型的训练集，也可以使用独立的“transfer set”.大模型可以是单个大模型，也可以是集成模型的概率平均。“soft targets”往往信息熵比较高，所以用它来训练小网络，它需要较少的数据和训练时间，可以使用一个比较高的学习率，算法示意图如下。</p>
</li>
</ul>
<p><a href="https://camo.githubusercontent.com/cfee36c2ed92f4515c856267d1815cb1632312a7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f353532393939372d336566303536356132313565333966382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/cfee36c2ed92f4515c856267d1815cb1632312a7/68747470733a2f2f75706c6f61642d696d616765732e6a69616e7368752e696f2f75706c6f61645f696d616765732f353532393939372d336566303536356132313565333966382e706e673f696d6167654d6f6772322f6175746f2d6f7269656e742f" alt="img"></a></p>
<p>后续待补。。。。。。。。。。。。。。。。</p>
<h3 id="2-2015-ICLR-FitNets-Hints-for-Thin-Deep-Nets"><a href="#2-2015-ICLR-FitNets-Hints-for-Thin-Deep-Nets" class="headerlink" title="2. 2015-ICLR-FitNets:Hints for Thin Deep Nets"></a>2. 2015-ICLR-<a href="https://arxiv.org/pdf/1412.6550.pdf" target="_blank" rel="noopener">FitNets:Hints for Thin Deep Nets</a></h3><ul>
<li><p><strong>论文摘要：</strong></p>
<p>论文认为，deep是DNN的主要的功效的来源，于是这篇文章的主要目的是去mimic一个更深但是更小的网络而且准确率比原始网络还要好。那么既然网络很深直接训练会困难，那就通过在中间层加入loss的方法，将网络分成两块来训练。中间的loss则通过teacher的feature map得到。</p>
</li>
<li><p><strong>数据集:</strong> CIFAR-10,CIFAR-100,SVHN,AFLW</p>
</li>
<li><p><strong>基础网络：</strong> Maxout networks</p>
</li>
<li><p><strong>方法：</strong> </p>
<p>两阶段法：先用hint training 去pretrain小模型前半部分参数，再用KD Training去训练全体参数。</p>
<ol>
<li><p>Teacher网络的某一中间层的权值为$W_t = W_{hint}$ ,Student 网络的某一中间层的权值为$W_s = W_{guided}$ .使用一个映射函数$W_r$ 来使得$W_{guided}$ 的维度匹配$W_{hint}$ ,得到$W_{s’}$ 。其中对于$W_r$的训练使用MSEloss.</p>
<script type="math/tex; mode=display">
\mathcal{L}_{H T}\left(\mathbf{W}_{\mathrm{Guided}}, \mathbf{W}_{\mathbf{r}}\right)=\frac{1}{2}\left\|u_{h}\left(\mathbf{x} ; \mathbf{W}_{\mathrm{Hint}}\right)-r\left(v_{g}\left(\mathbf{x} ; \mathbf{W}_{\mathrm{G} \text { uided }}\right) ; \mathbf{W}_{\mathbf{r}}\right)\right\|^{2}</script></li>
<li><p>使用hinton的KD方法训练整个网络</p>
<script type="math/tex; mode=display">
\mathrm{P}_{\mathrm{T}}^{\tau}=\operatorname{softmax}\left(\frac{\mathrm{a}_{T}}{\tau}\right), \quad \mathrm{P}_{\mathrm{S}}^{\tau}=\operatorname{softmax}\left(\frac{\mathrm{a}_{S}}{\tau}\right)</script><script type="math/tex; mode=display">
\mathcal{L}_{K D}\left(\mathbf{W}_{\mathbf{S}}\right)=\mathcal{H}\left(\mathbf{y}_{\text { true }}, \mathbf{P}_{\mathrm{S}}\right)+\lambda \mathcal{H}\left(\mathrm{P}_{\mathrm{T}}^{\tau}, \mathrm{P}_{\mathrm{S}}^{\tau}\right)</script></li>
</ol>
</li>
<li><p><strong>实验结果：</strong> </p>
<p>这里只放一个cifar-10的实验作为说明。</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57hn81kbwj31am0ig79m.jpg" alt="image-20190721152341678"></p>
</li>
<li><p>复现性：开源<a href="https://github.com/adri-romsor/FitNets" target="_blank" rel="noopener">Theano</a></p>
</li>
</ul>
<h3 id="3-2017-ICLR-Paying-More-Attention-Improving-the-Performance-of-Convolutional-Neural-Networks-via-Attention-Transfer"><a href="#3-2017-ICLR-Paying-More-Attention-Improving-the-Performance-of-Convolutional-Neural-Networks-via-Attention-Transfer" class="headerlink" title="3. 2017-ICLR-Paying More Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer"></a>3. 2017-ICLR-<a href="https://arxiv.org/abs/1612.03928" target="_blank" rel="noopener">Paying More Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</a></h3><ul>
<li><p><strong>论文摘要：</strong></p>
<p>模仿教师网络的attention map</p>
</li>
<li><p><strong>数据集：</strong> CIFAR，Imagenet</p>
</li>
<li><p><strong>基础网络结构：</strong> Network in Network, ResNet</p>
</li>
<li><p><strong>方法：</strong> </p>
</li>
<li><p>通过网络中间层的attention map，完成teacher network与student network之间的知识迁移。考虑给定的tensor A，基于activation的attention map可以定义为如下三种之一：</p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79ly1g57jpyppefj318g070aam.jpg" alt="image-20190721163509466"></p>
<p>随着网络层次的加深，关键区域的attention-level也随之提高。文章最后采用了第二种形式的attention map，取p=2，并且activation-based attention map的知识迁移效果优于gradient-based attention map，loss定义及迁移过程如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{A T}=\mathcal{L}\left(\mathbf{W}_{S}, x\right)+\frac{\beta}{2} \sum_{j \in \mathcal{I}}\left\|\frac{Q_{S}^{j}}{\left\|Q_{S}^{j}\right\|_{2}}-\frac{Q_{T}^{j}}{\left\|Q_{T}^{j}\right\|_{2}}\right\|_{p}</script><script type="math/tex; mode=display">
Q_{S}^{j}=\operatorname{vec}\left(F\left(A_{S}^{j}\right)\right) \text { and } Q_{T}^{j}=\operatorname{vec}\left(F\left(A_{T}^{j}\right)\right)</script><p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57jurpzcqj31bq0h0jtv.jpg" alt="image-20190721164008950"></p>
</li>
<li><p><strong>实验结果：</strong> </p>
<p>cifar-10</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57k0b2rvtj31ba0bggms.jpg" alt="image-20190721164521204"></p>
</li>
<li><p><strong>复现性</strong>：<a href="https://github.com/szagoruyko/attention-transfer" target="_blank" rel="noopener">开源</a> </p>
</li>
</ul>
<h3 id="4-2017-CVPR-A-Gift-from-Knowledge-Distillation-Fast-Optimization-Network-Minimization-and-Transfer-Learning"><a href="#4-2017-CVPR-A-Gift-from-Knowledge-Distillation-Fast-Optimization-Network-Minimization-and-Transfer-Learning" class="headerlink" title="4. 2017-CVPR-A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning"></a>4. 2017-CVPR-<a href="https://zpascal.net/cvpr2017/Yim_A_Gift_From_CVPR_2017_paper.pdf" target="_blank" rel="noopener">A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning</a></h3><ul>
<li><p><strong>论文摘要：</strong> </p>
<p>定义需要从教师网络学习的知识，为flow between layers, 通过两层之间的内积来计算。</p>
</li>
<li><p><strong>数据集：</strong> CIFAR-10, CIFAR-100</p>
</li>
<li><p><strong>基础网路：</strong> res-net</p>
</li>
<li><p><strong>方法：</strong></p>
<p>需要学习的知识可表示为训练的求解过程（FSP: Flow of the Solution Procedure），教师网络或学生网络的FSP矩阵定义如下（Gram形式的矩阵）：</p>
<script type="math/tex; mode=display">
G_{i, j}(x ; W)=\sum_{s=1}^{h} \sum_{t=1}^{w} \frac{F_{s, t, i}^{1}(x ; W) \times F_{s, t, j}^{2}(x ; W)}{h \times w}</script><p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57ktjamrvj30sy0g2mzr.jpg" alt="image-20190721171333348">p</p>
</li>
</ul>
<p>训练第一阶段：最小化教师网络FSP矩阵和学生网络FSP矩阵之间的L2 Loss.</p>
<script type="math/tex; mode=display">\begin{array}{l}{L_{F S P}\left(W_{t}, W_{s}\right)}  {=\frac{1}{N} \sum_{x} \sum_{i=1}^{n} \lambda_{i} \times\left\|\left(G_{i}^{T}\left(x ; W_{t}\right)-G_{i}^{S}\left(x ; W_{s}\right) \|_{2}^{2}\right.\right.}\end{array}</script><p>训练第二阶段：在目标任务的数据集上fine-tune学生网络。从而达到知识迁移，快速收敛，以及迁移学习的目的。</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57lki10knj31gs0u0tgk.jpg" alt="image-20190721173928639"></p>
<ul>
<li><p><strong>实验结果：</strong></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57lwhafgbj30tk0cs763.jpg" style="zoom:50%"></p>
<p><img src="http://ww4.sinaimg.cn/large/006tNc79ly1g57ly168sdj30t60ck0uj.jpg" style="zoom:50%"></p>
</li>
<li><p>复现性：h</p>
</li>
</ul>
<h3 id="5-2017-arXiv-Like-What-You-Like-Knowledge-Distill-via-Neuron-Selectivity-Transfer-图森"><a href="#5-2017-arXiv-Like-What-You-Like-Knowledge-Distill-via-Neuron-Selectivity-Transfer-图森" class="headerlink" title="5.2017-arXiv-Like What You Like: Knowledge Distill via Neuron Selectivity Transfer(图森)"></a>5.<a href="https://arxiv.org/pdf/1707.01219.pdf" target="_blank" rel="noopener">2017-arXiv-Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</a>(图森)</h3><ul>
<li><p><strong>论文摘要</strong></p>
<p>该论文主要引入了最大平均差异 Maximum Mean Discrepancy(MMD),来计算教师网络和学生网络的特征分布差异，并与原始损失函数结合，提高学生网络的性能。(定义新的知识，神经元选择性NST，或者激活值模式，反映了神经元的特征选择。)</p>
</li>
<li><p><strong>数据集</strong></p>
<p>CIFAR-10,CIFAR-100, ImageNet LSVRC2012.</p>
</li>
<li><p><strong>方法</strong></p>
<p>Neuron Selectivity Transfer (NST)</p>
<p>为什么不直接匹配feature maps?因为它忽视了空间中样本的密度。</p>
<p>NST Loss的定义为：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{NST}}\left(\mathbf{W}_{S}\right)=\mathcal{H}\left(\boldsymbol{y}_{\text { true }}, \boldsymbol{p}_{S}\right)+\frac{\lambda}{2} \mathcal{L}_{\mathrm{MMD}^{2}}\left(\mathbf{F}_{T}, \mathbf{F}_{S}\right)</script><p>$\mathcal{H}$ 表示标准交叉熵损失函数。</p>
</li>
</ul>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51k7g3xa9j313o0l60tr.jpg" style="zoom:50%"></p>
<p>​       其中k代表核函数，本文测试了3中核函数：</p>
<p>​       线性核：$k(\boldsymbol{x}, \boldsymbol{y})=\boldsymbol{x}^{\top} \boldsymbol{y}$，线性核可以反映神经元对哪些特征反映强烈，即注意力。</p>
<p>​      多项式核：$k(\boldsymbol{x}, \boldsymbol{y})=\left(\boldsymbol{x}^{\top} \boldsymbol{y}+c\right)^{d}$ 多项式核反映了区域相似性。</p>
<p>​      高斯核：$k(\boldsymbol{x}, \boldsymbol{y})=\exp \left(-\frac{|\boldsymbol{x}-\boldsymbol{y}|_{2}^{2}}{2 \sigma^{2}}\right)$</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53ren7gzuj31cm0lgdh1.jpg" style="zoom:50%"></p>
<ul>
<li><p><strong>实验结果</strong></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51mqkr1wzj30yo0kiaek.jpg" style="zoom: 50%"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51mrmkldaj30nc0fkwhj.jpg" style="zoom:50%"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g549nghgotj31am0bygoy.jpg" alt="屏幕快照 2019-07-16 下午1.52.01"></p>
</li>
</ul>
<h3 id="6-2017-DarkRank-Acclerating-Deep-Metric-Learning-Via-Cross-Sample-Similarities-Transfer（图森）"><a href="#6-2017-DarkRank-Acclerating-Deep-Metric-Learning-Via-Cross-Sample-Similarities-Transfer（图森）" class="headerlink" title="6. 2017-DarkRank:Acclerating Deep Metric Learning Via Cross Sample Similarities Transfer（图森）"></a>6. 2017-DarkRank:Acclerating Deep Metric Learning Via Cross Sample Similarities Transfer（图森）</h3><ul>
<li><p><strong>论文摘要：</strong></p>
<p>改论文定义的需要从教师网络学习到的知识为样本间的相似性排序融入到监督训练中，并融合了softmax,Verify loss, triplet loss共同训练。</p>
</li>
<li><p><strong>数据集：</strong> CUHK03, Market1501,CUB-200-2011</p>
</li>
<li><p><strong>网络模型：</strong> Inception-BN, NIN-BN</p>
<p>方法：</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57s3augprj30y10jkjtz.jpg" alt="img"></p>
</li>
<li><p>实验结果：</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57sj9fiaej30tg0nogpq.jpg" style="zoom: 50%"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57slsxg8wj30ty0hwtba.jpg" style="zoom:50%"></p>
</li>
<li><p>复现性：开源<a href="https://github.com/TuSimple/DarkRank" target="_blank" rel="noopener">MxNet</a></p>
</li>
</ul>
<h3 id="7-2019-CVPR-Relational-Knowledge-Distillation"><a href="#7-2019-CVPR-Relational-Knowledge-Distillation" class="headerlink" title="7.  2019-CVPR-Relational Knowledge Distillation"></a>7.  <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Relational_Knowledge_Distillation_CVPR_2019_paper.pdf" target="_blank" rel="noopener">2019-CVPR-Relational Knowledge Distillation</a></h3><ul>
<li><p><strong>论文摘要：</strong> </p>
<p>本文定义了一种新的需要从教师网络学习的知识，即样本之间的关系，（relational knowledge distillation, RKD).提出了两种衡量样本关系的方式，distance-wise, angel-wise.</p>
</li>
<li><p><strong>数据集：</strong> CIFAR-100, Tiny ImageNet.</p>
</li>
<li><p><strong>基础网络：</strong> Resnet50, VGG11</p>
</li>
<li><p><strong>方法：</strong> </p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57mvel4z0j30q40m0wkr.jpg" style="zoom:50%"> </p>
<p>定义新的损失函数：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{RKD}}=\sum_{\left(x_{1}, \ldots, x_{n}\right) \in \mathcal{X}^{N}} l\left(\psi\left(t_{1}, . ., t_{n}\right), \psi\left(s_{1}, \dots, s_{n}\right)\right)</script><p>其中$（x_1,x_2,…x_n）$ 是 n个样本组成的关系元组。$\psi$ 是relational potential function.</p>
<ul>
<li><p>Distance wise distillation loss</p>
<script type="math/tex; mode=display">
\psi_{\mathrm{D}}\left(t_{i}, t_{j}\right)=\frac{1}{\mu}\left\|t_{i}-t_{j}\right\|_{2}</script></li>
</ul>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{RKD}-\mathrm{D}}=\sum_{\left(x_{i}, x_{j}\right) \in \mathcal{X}^{2}} l_{\delta}\left(\psi_{\mathrm{D}}\left(t_{i}, t_{j}\right), \psi_{\mathrm{D}}\left(s_{i}, s_{j}\right)\right)</script><p>  Huber Loss</p>
<script type="math/tex; mode=display">
l_{\delta}(x, y)=\left\{\begin{array}{ll}{\frac{1}{2}(x-y)^{2}} & {\text { for }|x-y| \leq 1} \\ {|x-y|-\frac{1}{2},} & {\text { otherwise }}\end{array}\right.</script></li>
<li><p>Angle-wise distillation loss</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\psi_{\mathrm{A}}\left(t_{i}, t_{j}, t_{k}\right)=\cos \angle t_{i} t_{j} t_{k}=\left\langle\mathbf{e}^{i j}, \mathbf{e}^{k j}\right\rangle} \\ {\text { where } \quad \mathbf{e}^{i j}=\frac{t_{i}-t_{j}}{\left\|t_{i}-t_{j}\right\|_{2}}, \mathbf{e}^{k j}=\frac{t_{k}-t_{j}}{\left\|t_{k}-t_{j}\right\|_{2}}}\end{array}</script><script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{RKD}-\mathrm{A}}=\sum_{\left(x_{i}, x_{j}, x_{k}\right) \in \mathcal{X}^{3}} l_{\delta}\left(\psi_{\mathrm{A}}\left(t_{i}, t_{j}, t_{k}\right), \psi_{\mathrm{A}}\left(s_{i}, s_{j}, s_{k}\right)\right)</script></li>
<li><p>实验结果</p>
<p>teacher:resnet-50, student:VGG11</p>
<p>teacher:resnet-101, resnet-18</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g57qrao0jqj30u40i4t9x.jpg" style="zoom: 50%"> </p>
</li>
<li><p>复现难度：h</p>
</li>
</ul>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><h3 id="1-2017-NIPS-Learning-Efficient-Object-Detection-Models-with-Knowledge-Distillation（博客解读）"><a href="#1-2017-NIPS-Learning-Efficient-Object-Detection-Models-with-Knowledge-Distillation（博客解读）" class="headerlink" title="1.2017-NIPS-Learning Efficient Object Detection Models with Knowledge Distillation（博客解读）"></a>1.<a href="http://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation.pdf" target="_blank" rel="noopener">2017-NIPS-Learning Efficient Object Detection Models with Knowledge Distillation</a>（<a href="https://blog.csdn.net/nature553863/article/details/82463249" target="_blank" rel="noopener">博客解读</a>）</h3><ul>
<li><p><strong>论文摘要</strong></p>
<p>本文主要工作是将Knowledge Distillation和Hint Learning技术用在了目标检测模型Faster-RCNN上，提出了weighted cross-entropy来解决类别不平衡问题，使用teacher bounded loss解决bounding box 的回归问题，使用hint learning来学习教师网络的层间特征。</p>
</li>
<li><p><strong>数据集</strong></p>
<p>PASCAL, KITTI, ILSVRC, MS-COCO</p>
</li>
<li><p><strong>方法</strong></p>
<p>学生网络的整体目标函数：</p>
<script type="math/tex; mode=display">
\begin{aligned} L_{R C N} &=\frac{1}{N} \sum_{i} L_{c l s}^{R C N}+\lambda \frac{1}{N} \sum_{j} L_{r e g}^{R C N} 
\\\\ L_{R P N} &=\frac{1}{M} \sum_{i} L_{c l s}^{R P N}+\lambda \frac{1}{M} \sum_{j} L_{r e g}^{R P N} 
\\\\ L &=L_{R P N}+L_{R C N}+\gamma L_{H i n t} \end{aligned}</script><p>$L_{R C N},L_{RPN},L_{Hint}$分别代表RCN网络，RPN网络和Hint-based损失函数。</p>
<ol>
<li><p>对于分类的知识蒸馏（with Imabalanced Classes）</p>
<script type="math/tex; mode=display">
L_{c l s}=\mu L_{h a r d}\left(P_{s}, y\right)+(1-\mu) L_{s o f t}\left(P_{s}, P_{t}\right)</script><script type="math/tex; mode=display">
L_{s o f t}\left(P_{s}, P_{t}\right)=-\sum w_{c} P_{t} \log P_{s}</script><p>其中$P_t=softmax(\frac{Z_t}{T})$ 表示教师网络的输出，T是一个温度参数，这里通常设为1.$P_{s}=\operatorname{softmax}\left(\frac{Z_{s}}{T}\right)$ 表示学生网络的输出。$L_{cls}$为groundtrues类别标签.这里的损失函数都采用的交叉熵，为了解决目标检测任务中类别不平衡的问题，引入了加权交叉熵，对背景类采用较大的权重，因为背景类在实验过程中，分错的概率较小。</p>
</li>
<li><p>对于回归知识的蒸馏（with Teacher Bounds）</p>
<script type="math/tex; mode=display">
L_{b}\left(R_{s}, R_{t}, y\right) =\left\{\begin{array}{ll}{\left\|R_{s}-y\right\|_{2}^{2},}   {\text { if }\left\|R_{s}-y\right\|_{2}^{2}+m>\left\|R_{t}-y\right\|_{2}^{2}} \\ {0,}  {\text { otherwise }}\end{array}\right.</script><script type="math/tex; mode=display">
L_{\text {reg}} =L_{s L 1}\left(R_{s}, y_{\text {reg}}\right)+\nu L_{b}\left(R_{s}, R_{t}, y_{\text {reg}}\right)</script><p>对于boundingbox的回归不同于对离散类别信息的蒸馏，它本来就是连续的。而教师网络提供的回归方向有可能和groundtruth的方向是相反的，所以这里将教师的信息，作为学生网络回归的上边界，当超过一定距离时，才对学生网络提供监督。</p>
</li>
<li><p>Hint Learning with Feature Adaptation</p>
<p>即让学生网络去学习教师网络中间层特征图的分布，可以通过计算$L_1和L_2$loss实现。</p>
<script type="math/tex; mode=display">
L_{H i n t}(V, Z)=\|V-Z\|_{2}^{2}</script><script type="math/tex; mode=display">
L_{H i n t}(V, Z)=\|V-Z\|_{1}</script></li>
</ol>
</li>
<li><p><strong>实验结果</strong></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51hrfjvvaj312y0eoq7c.jpg" alt="屏幕快照 2019-07-16 上午10.54.04"></p>
<p>下图是使用高分辨率图片训练教师网络，低分辨率图片训练学生网络的结果。</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g51i305natj31cu0cqjv1.jpg" alt="屏幕快照 2019-07-16 上午11.05.15"></p>
</li>
</ul>
<h3 id="2-2017-商汤-Mimicking-Very-Efficient-Network-for-Object-Detection"><a href="#2-2017-商汤-Mimicking-Very-Efficient-Network-for-Object-Detection" class="headerlink" title="2. 2017-商汤-Mimicking Very Efficient Network for Object Detection"></a>2. <a href="http://xueshu.baidu.com/usercenter/paper/show?paperid=29f43e4a7e341d555f0f95566bde9cd4&amp;site=xueshu_se" target="_blank" rel="noopener">2017-商汤-Mimicking Very Efficient Network for Object Detection</a></h3><ul>
<li><p>论文摘要：</p>
<p>论文针对目标检测模型，论文提出了一种针对特征图Mimicking(蒸馏)的方法，主要针对localregion进行蒸馏，而不是全局特征。小型化的Inception取得了2.5倍的压缩，并取得和原始模型相当的准确度。在Caltech数据集上处理1000x1500的输入可以达到80FPS。</p>
</li>
<li><p>数据集：Caltech, Pascal VOC</p>
</li>
<li><p>基础模型：Inception, ResNet，Faser-rcnn, R-FCN.</p>
</li>
<li><p>方法：</p>
<ol>
<li><p>目标检测产生的特征图往往维度比较高，难以直接对两个feature map进行回归。而且对于目标检测特征图中，只有object区域的相应比较高，背景区域都是噪声。所以在特征图中，只有目标附近区域（local regions）才包含较多的有用信息。只针对RPN网络。</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathcal{L}(W) &=\lambda_{1} \mathcal{L}_{m}(W)+\mathcal{L}_{g t}(W) \\ \mathcal{L}_{m}(W) &=\frac{1}{2 N} \sum_{i}\left\|u^{(i)}-r\left(v^{(i)}\right)\right\|_{2}^{2} \\ \mathcal{L}_{g t}(W) &=\mathcal{L}_{c l s}(W)+\lambda_{2} \mathcal{L}_{r e g}(W) \end{aligned}</script><p>$\mathcal{L}(W)$ 表示特征图模仿的$L_2$ 损失，N表示region proposal的数量，$u^{i}$表示教师网络产生的第i个proposal经过spp处理后得到的特征，$v{i}$同理，r是一个回归函数用来统一两者的维度。</p>
<p>这样还存在问题，一是特征图的的值可能较大，需要小心平衡，groundtruth损失和mimic损失，即$\lambda$ 参数；二是spp可能会破坏特征。所以进行了归一化改进，和去除spp,不同rp对应的特征图的大小是不同的。.</p>
<script type="math/tex; mode=display">
\mathcal{L}_{m}(W)=\frac{1}{2 N} \sum_{i} \frac{1}{m_{i}}\left\|u^{(i)}-r\left(v^{(i)}\right)\right\|_{2}^{2}</script></li>
</ol>
</li>
</ul>
<ol>
<li><p>Two-stage Mimic: 为了提高准确率除了对RPN网络进行Mimic,对detecter网络也进行Mimic.</p>
</li>
<li><p>Mimic over Scales:使用大图片去训练教师网络，使用小图片训练学生网络</p>
</li>
</ol>
<ul>
<li><p>实验结果</p>
<p>Caltech,评价指标：log average missrate on FPPI</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yo475yaj30sc0ccq51.jpg" style="zoom:50%"></p>
</li>
</ul>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yqax3mlj30u20n2tci.jpg" style="zoom:50%"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53ytkx7q6j30te0isgp2.jpg" style="zoom:50%"></p>
<p>减小输入分辨率：</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5c68zwqb9j30io08k3zt.jpg" style="zoom:50"></p>
<p>Pascal VOC 评价指标：mAP</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yzm5xpwj30u00c0tam.jpg" style="zoom:50%"></p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g53yzxrpbej30sm0gk76z.jpg" style="zoom:50%"></p>
<ul>
<li>复现难度：m</li>
</ul>
<h3 id="3-2018-商汤-CVPR-2019-Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection"><a href="#3-2018-商汤-CVPR-2019-Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection" class="headerlink" title="3. 2018-商汤-CVPR-2019-Quantization Mimic: Towards Very Tiny CNN for Object Detection"></a>3. <a href="https://arxiv.org/pdf/1805.02152.pdf" target="_blank" rel="noopener">2018-商汤-CVPR-2019-Quantization Mimic: Towards Very Tiny CNN for Object Detection</a></h3><ul>
<li><p>论文摘要：本文主要目标是训练VeryTiny网络，（VGG，1/32）.提出了Quantization Mimic方法来蒸馏网络，首先量化教师网络，再将学生网络使用蒸馏方法Mimic教师网络。量化教师网络可以降低学生网络的参数搜索空间，别人使用量化直接压缩网络，作者使用量化来辅助蒸馏。</p>
</li>
<li><p>数据集：WIDER FACE， PascalVOC</p>
</li>
<li><p>基础网络：VGG with R-FCN，ResNet with Faster-R-CNN.</p>
</li>
<li><p>方法：</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g540ls258aj31820tsgrz.jpg" alt="image-20190718151724599"></p>
<ol>
<li><p>量化：对教师网络的最后一层特征进行量化，使用Uniform量化，因为他更适用于当前模型。</p>
<p>量化函数$Q$为：</p>
<script type="math/tex; mode=display">
Q(f)=\beta \quad \text { if } \frac{\alpha+\beta}{2}<f \leq \frac{\gamma+\beta}{2}</script><p>其中，$\alpha,\beta, \gamma$ 是在量化字典中相邻的元素：</p>
<script type="math/tex; mode=display">
D=\{0, s, 2 s, 3 s . .\}</script><p>s是量化步长。</p>
<p>需要说明的是，在训练的时候类似于BNN，使用的是全精度梯度。</p>
</li>
<li><p>Mimic:采用和上一篇论文一样的方法</p>
<script type="math/tex; mode=display">
\begin{array}{c}{L=L_{c l s}^{r}+L_{r e g}^{r}+L_{c l s}^{d}+L_{r e g}^{d}+\lambda L_{m}} \\ {L_{m}=\frac{1}{2 N} \sum_{i}\left\|f_{t}^{i}-r\left(f_{s}^{i}\right)\right\|_{2}^{2}}\end{array}</script></li>
</ol>
</li>
</ul>
<ol>
<li>Quantization Mimic:<br>  $<br>  L_{m}=\frac{1}{2 N} \sum_{i}\left|Q\left(f_{t}^{i}\right)-Q\left(r\left(f_{s}^{i}\right)\right)\right|_{2}^{2}<br>  $<br>  <img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5417b20v3j31180scn42.jpg" alt="image-20190718153806956"></li>
</ol>
<ul>
<li><p><a href="https://blog.csdn.net/bryant_meng/article/details/83056203" target="_blank" rel="noopener">CSDN解读</a></p>
</li>
<li><p>实验结果：</p>
<ol>
<li><p>WIDER FACE Dataset</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g541jdkwhrj30v70u07bl-20220717190330356.jpg" alt="image-20190718154943309"></p>
</li>
</ol>
</li>
</ul>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g541lu1m7dj310c0e076z.jpg" style="zoom:50%"></p>
<ol>
<li><p>Pascal VOC</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g541o5xe3zj31040cu76i.jpg" alt="image-20190718155419344"></p>
<p>效果有点儿差哦,有一定的参考价值。</p>
</li>
</ol>
<ul>
<li>复现难度：h </li>
</ul>
<h3 id="4-2019-华为诺亚方舟-Distilling-Object-Detectors-with-Fine-grained-Feature-Imitation"><a href="#4-2019-华为诺亚方舟-Distilling-Object-Detectors-with-Fine-grained-Feature-Imitation" class="headerlink" title="4. 2019-华为诺亚方舟-Distilling Object Detectors with Fine-grained Feature Imitation"></a>4. 2019-华为诺亚方舟-<a href="https://arxiv.org/pdf/1906.03609.pdf" target="_blank" rel="noopener">Distilling Object Detectors with Fine-grained Feature Imitation</a></h3><ul>
<li><p>论文摘要</p>
<p>本文的思想和商汤的第一篇论文很相似，认为在目标检测模型的模型蒸馏中不能单纯的mimic教师网络的整个特征图，这样会引入很多噪声，目标附近的区域才是比较重要的，但是本文不是去mimic region proposal,而是事先根据gt和anchor计算出目标临近的区域。然后让学生去mimic这些区域的特征。</p>
</li>
<li><p>数据集：KITTI, Pascal VOC, COCO.</p>
</li>
<li><p>模型：toy-detector, faster R-CNN.</p>
</li>
<li><p>方法：</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g543cgs196j30to0si44j.jpg" style="zoom:50%"></p>
<ol>
<li><p>如图所示，首先计算特征图上每个点所有anchors和gtbox的IOU，然后计算出最大的IOU值，$M = max(m)$ .然后乘一个阈值因子$\psi$ ,然后得到一个阈值$F=\psi * M$ .用这个阈值去过滤IOU map,得到一个WXH的map,最后结合所有的gtbox得到一个mask I.</p>
</li>
<li><p>$N_p$是mask上正值得个数，$f_{adap}$ 是为了对齐两个网络的特征图。</p>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned} L_{i m i t a t i o n} &=\frac{1}{2 N_{p}} \sum_{i=1}^{W} \sum_{j=1}^{H} \sum_{c=1}^{C} I_{i j}\left(f_{\mathrm{adap}}(s)_{i j c}-t_{i j c}\right)^{2} \\ \text { where } N_{p} &=\sum_{i=1}^{W} \sum_{j=1}^{H} I_{i j} \end{aligned}</script><p>总的损失函数为：</p>
</li>
</ol>
</li>
</ul>
<script type="math/tex; mode=display">
L=L_{g t}+\lambda L_{i m i t a t i o n}</script><ul>
<li><p>实验结果：</p>
<ol>
<li><p>toy-detector</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5449r87amj31mo0q6wh0.jpg"></p>
</li>
</ol>
</li>
</ul>
<ol>
<li><p>Faster R-CNN</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g544fo2zu5j31iq0u0qay.jpg" alt="image-20190718172955271"></p>
</li>
</ol>
<ul>
<li>复现难度：<a href="https://github.com/twangnh/Distilling-Object-Detectors" target="_blank" rel="noopener">开源</a></li>
</ul>
<h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><h3 id="1-2016-AAAI-汤晓鸥组-Face-Model-Compression-by-Distilling-Knowledge-from-Neurons"><a href="#1-2016-AAAI-汤晓鸥组-Face-Model-Compression-by-Distilling-Knowledge-from-Neurons" class="headerlink" title="1.2016-AAAI-汤晓鸥组-Face Model Compression by Distilling Knowledge from Neurons"></a>1.<a href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/aaai16-face-model-compression.pdf" target="_blank" rel="noopener">2016-AAAI-汤晓鸥组-Face Model Compression by Distilling Knowledge from Neurons</a></h3><ul>
<li><p><strong>论文摘要：</strong> </p>
<p>不同于Hinton使用“soft target”作为需要学习的知识，本文使用高层的神经元作为学习的知识，它含有和输出概率同等的信息，但是更加的compact(坚实，紧凑)，而且使用“soft target”的话不容易拟合。利用学习到的人脸的基本特点(领域知识)，提出了一个神经元选择方法选择出和人脸识别更相关的神经元。使用选择出来的神经元作为监督信息来模仿DeepID2+和DeepID3。并在LFW上取得了比教师网络的更高的精度。当使用一个DeepID2+的集成网络时（6个网络），学生网络可以取得51.6倍的压缩比，和90倍的推理速度提升。AUC为98.43。</p>
</li>
<li><p><strong>数据集：</strong> training: WDRef, CelebFaces+; testing: LFW</p>
</li>
<li><p><strong>基础网络：</strong> DeepID2+, DeepID3</p>
</li>
<li><p><strong>方法：</strong> </p>
<ol>
<li><p>神经元选择方法基于三个original observations(domain knowledge):</p>
<ul>
<li>深度学习学习到的人脸特征是人脸属性的分布特征（distributed representation over face attributes),包括身份有关 属性(IA)，比如性别，种族。和身份无关属性(NA)，比如表情，光照，照片质量等。在训练过程中尽管没有提供这些属性信息，但是可以发现某个神经元和某些属性是有联系的。</li>
<li>这些分布式特征既不是不变的，也不是完全分离的（neither invariant nor completely factorized）.应该将与NA有关的神经元移除。</li>
<li>有些神经元一直处于抑制状态，是噪音。</li>
</ul>
<p>一个平均场算法（mean field algorithm),可以让我们选出与IA有关的神经元，但是相互关系较少的神经元。</p>
</li>
<li><p>使用Neuron Selection来训练学生网络</p>
<script type="math/tex; mode=display">
L(\mathcal{D})=\frac{1}{2 M} \sum_{i=1}^{M}\left\|\mathbf{f}_{i}-g\left(\mathbf{I}_{i} ; \mathbf{W}\right)\right\|_{2}^{2}</script><p>上式为训练学生网络的损失函数，其中$f_i$为地i个图像通过神经元选择方法筛选出来的特征。</p>
</li>
<li><p>神经元如何选择？</p>
<p>将神经元之间的关系看成一个全连接图问题，比如N个Neuron $\mathbf{y}=\left\{y_{i}\right\}_{i=1}^{N}$,$y_{i} \in\{0,1\}$ .</p>
<p>通过最小化下面的能量函数来实现神经元的选择：</p>
<script type="math/tex; mode=display">
E(\mathbf{y})=\sum_{i=1}^{N} \Phi\left(y_{i}\right)+\lambda \sum_{i=1}^{N} \sum_{j=1, j \neq i}^{N} \Psi\left(y_{i}, y_{j}\right)</script><p>其中$\Phi(y_{i})$ 和 $\Psi(y_i,y_j)$ 分别 表示选中神经元i的损失，和同时选中i，j的损失。</p>
<p>$\Phi(x_i) = f(x_i)$ ,$f(.)$ 是一个惩罚函数，$x_i$ 是一个向量，用来表示神经元i的特征区分能力。</p>
<p>$\Psi(.)$ 也是一个惩罚函数，用来惩罚相关性较大的神经元。</p>
<script type="math/tex; mode=display">
\Psi\left(y_{i}, y_{j}\right)=\exp \left\{-\frac{1}{2}\left\|\mathbf{x}_{i}-\mathbf{x}_{j}\right\|_{2}^{2}\right\}</script><p>这个能量函数可以用平均场算法求解。</p>
</li>
<li><p>$x_i$ 怎么求得？</p>
<p>$x_i$的每一维表示第i个神经元，对第j个特征的分类准确率：</p>
<script type="math/tex; mode=display">
\text { i.e. } \forall \mathbf{x}_{i} \in \mathbb{R}^{1 \times 40} \text { and } \mathbf{x}_{i(j)}=\frac{T P_{j}+T N_{j}}{2}</script><p>其中，$TP_j和TN_j$ 表示真正率和真假率。至于这个怎么统计的，文中没有提，我觉得因该是根据某个样本的属性，看这个神经元对这个样本有没有反应来统计。</p>
<p>最后，得到$f(.)$的最终表达式：</p>
<script type="math/tex; mode=display">
f\left(\mathbf{x}_{i}\right)=\frac{\max \left\{\mathbf{x}_{i(j)}\right\} \forall j \in \mathrm{NA}-\operatorname{avg}\left\{\mathbf{x}_{i(j)}\right\} \forall j \in \mathrm{NA}}{\max \left\{\mathbf{x}_{i(j)}\right\} \forall j \in \mathrm{IA}-\operatorname{avg}\left\{\mathbf{x}_{i(j)}\right\} \forall j \in \mathrm{IA}}</script><p>如果一个神经元对NA属性的选择性大于对IA的选择性，他就会受到惩罚。</p>
</li>
</ol>
</li>
<li><p>实验结果</p>
<p>在LFW数据集上进行人脸验证，人脸验证通过计算欧式距离来实现。评价指标AUC。</p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g54zjxb288j31800d2dhw.jpg" alt="image-20190719112633318"></p>
</li>
</ul>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g54zkhl1dhj30nc0pmq64.jpg" style="zoom:50%"></p>
<ul>
<li><p>复现难度：<a href="https://github.com/liuziwei7/mobile-id" target="_blank" rel="noopener">开源</a></p>
<hr>
</li>
</ul>
<h3 id="2-2019-arXiv-ICML-Triple-Distillation-for-Deep-Face-Recognition"><a href="#2-2019-arXiv-ICML-Triple-Distillation-for-Deep-Face-Recognition" class="headerlink" title="2. 2019-arXiv-ICML-Triple Distillation for Deep Face Recognition"></a>2. <a href="https://arxiv.org/abs/1905.04457?context=cs.CV" target="_blank" rel="noopener">2019-arXiv-ICML-Triple Distillation for Deep Face Recognition</a></h3><ul>
<li><p><strong>论文摘要：</strong></p>
<p>本文主要把Triplet loss和蒸馏的思想进行了结合引入了Triplet-distillation.改进了Triplet-loss中，identities之间的固定间距。从训练好教师网络中学习indenties之间的多样性知识。</p>
</li>
<li><p><strong>数据集：</strong> LFW，AgeDB, CPLFW</p>
</li>
<li><p><strong>网络模型：</strong> ResNet-100, slim version of MobileFaceNet.</p>
</li>
<li><p><strong>方法：</strong> </p>
<p>  原始Triplet loss: </p>
<script type="math/tex; mode=display">\mathcal{L}=\frac{1}{N} \sum_{i}^{N} \max \left(\mathcal{D}\left(x_{i}^{a}, x_{i}^{p}\right)-\mathcal{D}\left(x_{i}^{a}, x_{i}^{n}\right)+m, 0\right)</script><p> 在原始Triplet loss中，对于所有的identities m是相同的且固定不变的，所有的聚簇都将使用固定的距离粗鲁的分开，它忽视了identities之间微妙的相似性。比如说A和B的相似性大于A和C的相似性，那么理论上{A,B}的m应该小于{B,C}的m.和hinton的思想一样的，这样的相似性是有用的。 <script type="math/tex">\begin{array}{c}{\mathcal{L}=\frac{1}{N} \sum_{i}^{N} \max \left(\mathcal{D}\left(x_{i}^{a}, x_{i}^{p}\right)-\mathcal{D}\left(x_{i}^{a}, x_{i}^{n}\right)+\mathcal{F}(d), 0\right)} \\ {d=\max \left(\mathcal{T}\left(x_{i}^{a}, x_{i}^{n}\right)-\mathcal{T}\left(x_{i}^{a}, x_{i}^{p}\right), 0\right)}\end{array}</script> </p>
<p>Triplet Distillation:</p>
<p>先训练一个教师网络，然后教师网络提取identities的特征，计算它们之间的距离。如上所示$\mathcal{D}$代表学生网络计算的距离，$\mathcal{T}$代表教师网络的距离。</p>
<p>$\mathcal{F}$是一个简单的线性函数: $\mathcal{F}(d)=\frac{m_{\max }-m_{\min }}{d_{\max }} d+m_{\min }$</p>
<p>通过这种方式m被限制在了$m_{min}$ 和 $m_{max}$ 之间。</p>
</li>
<li><p><strong>实验结果：</strong> </p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g555d2woo1j30j605474v.jpg" style="zoom:70%"></p>
<p><img src="http://ww3.sinaimg.cn/large/006tNc79ly1g555gehz77j30iu09m0u1.jpg" style="zoom:50%"></p>
</li>
<li><p>复现难度：将要开源</p>
</li>
</ul>
<h3 id="3-2019-arXiv-Deep-Face-Recognition-Model-Compression-via-Knowledge-Transfer-and-Distillation"><a href="#3-2019-arXiv-Deep-Face-Recognition-Model-Compression-via-Knowledge-Transfer-and-Distillation" class="headerlink" title="3.2019-arXiv-Deep Face Recognition Model Compression via Knowledge Transfer and Distillation"></a>3.2019-arXiv-<a href="https://arxiv.org/abs/1906.00619" target="_blank" rel="noopener">Deep Face Recognition Model Compression via Knowledge Transfer and Distillation</a></h3><ul>
<li><p><strong>论文摘要:</strong> 降低输入图像的大小来压缩网络，通过蒸馏方法来提高精度。</p>
</li>
<li><p><strong>数据集：</strong> LFW</p>
</li>
<li><p><strong>实验结果：</strong> </p>
<p><img src="https://xy-cloud-images.oss-cn-shanghai.aliyuncs.com/img/006tNc79ly1g5c44c953uj30j40ayabb.jpg" style="zoom:50%"></p>
</li>
</ul>
<h3 id="开源统计："><a href="#开源统计：" class="headerlink" title="开源统计："></a>开源统计：</h3><p><a href="https://github.com/RuiChen96/MaskRCNN-PyTorch" target="_blank" rel="noopener">MaskRcnn with Knowledge Distillation (pytorch)</a> </p>
<p><a href="https://github.com/HqWei/Distillation-of-Faster-rcnn" target="_blank" rel="noopener">Distillation-of-Faster-rcnn</a></p>
<p><a href="https://github.com/sseung0703/Knowledge_distillation_methods_wtih_Tensorflow" target="_blank" rel="noopener">各种蒸馏方法总结实验对比</a></p>
<p><a href="https://github.com/lenscloth/RKD" target="_blank" rel="noopener">Relational knowledge distillation</a></p>
<p><a href="https://github.com/irfanICMLL/structure_knowledge_distillation" target="_blank" rel="noopener">Structured Knowledge Distillation for Semantic Segmentation</a></p>
<p><a href="https://github.com/bhheo/AB_distillation" target="_blank" rel="noopener">Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons</a></p>
<p><a href="https://github.com/bhheo/BSS_distillation" target="_blank" rel="noopener">Knowledge Distillation with Adversarial Samples Supporting Decision Boundary (AAAI 2019)</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/我看的论文/" rel="tag"># 我看的论文</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/14/目标检测算法/" rel="next" title="目标检测算法">
                <i class="fa fa-chevron-left"></i> 目标检测算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/11/12/mtcnn/" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">xy</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#图像分类"><span class="nav-number">1.</span> <span class="nav-text">图像分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2015-NIPS-Distilling-the-Knowledge-in-a-Neural-Network"><span class="nav-number">1.0.1.</span> <span class="nav-text">1.2015-NIPS-Distilling the Knowledge in a Neural Network</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2015-ICLR-FitNets-Hints-for-Thin-Deep-Nets"><span class="nav-number">1.1.</span> <span class="nav-text">2. 2015-ICLR-FitNets:Hints for Thin Deep Nets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2017-ICLR-Paying-More-Attention-Improving-the-Performance-of-Convolutional-Neural-Networks-via-Attention-Transfer"><span class="nav-number">1.2.</span> <span class="nav-text">3. 2017-ICLR-Paying More Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2017-CVPR-A-Gift-from-Knowledge-Distillation-Fast-Optimization-Network-Minimization-and-Transfer-Learning"><span class="nav-number">1.3.</span> <span class="nav-text">4. 2017-CVPR-A Gift from Knowledge Distillation:Fast Optimization, Network Minimization and Transfer Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2017-arXiv-Like-What-You-Like-Knowledge-Distill-via-Neuron-Selectivity-Transfer-图森"><span class="nav-number">1.4.</span> <span class="nav-text">5.2017-arXiv-Like What You Like: Knowledge Distill via Neuron Selectivity Transfer(图森)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2017-DarkRank-Acclerating-Deep-Metric-Learning-Via-Cross-Sample-Similarities-Transfer（图森）"><span class="nav-number">1.5.</span> <span class="nav-text">6. 2017-DarkRank:Acclerating Deep Metric Learning Via Cross Sample Similarities Transfer（图森）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2019-CVPR-Relational-Knowledge-Distillation"><span class="nav-number">1.6.</span> <span class="nav-text">7.  2019-CVPR-Relational Knowledge Distillation</span></a></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#目标检测"><span class="nav-number">2.</span> <span class="nav-text">目标检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2017-NIPS-Learning-Efficient-Object-Detection-Models-with-Knowledge-Distillation（博客解读）"><span class="nav-number">2.1.</span> <span class="nav-text">1.2017-NIPS-Learning Efficient Object Detection Models with Knowledge Distillation（博客解读）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2017-商汤-Mimicking-Very-Efficient-Network-for-Object-Detection"><span class="nav-number">2.2.</span> <span class="nav-text">2. 2017-商汤-Mimicking Very Efficient Network for Object Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2018-商汤-CVPR-2019-Quantization-Mimic-Towards-Very-Tiny-CNN-for-Object-Detection"><span class="nav-number">2.3.</span> <span class="nav-text">3. 2018-商汤-CVPR-2019-Quantization Mimic: Towards Very Tiny CNN for Object Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2019-华为诺亚方舟-Distilling-Object-Detectors-with-Fine-grained-Feature-Imitation"><span class="nav-number">2.4.</span> <span class="nav-text">4. 2019-华为诺亚方舟-Distilling Object Detectors with Fine-grained Feature Imitation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#人脸识别"><span class="nav-number">3.</span> <span class="nav-text">人脸识别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2016-AAAI-汤晓鸥组-Face-Model-Compression-by-Distilling-Knowledge-from-Neurons"><span class="nav-number">3.1.</span> <span class="nav-text">1.2016-AAAI-汤晓鸥组-Face Model Compression by Distilling Knowledge from Neurons</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2019-arXiv-ICML-Triple-Distillation-for-Deep-Face-Recognition"><span class="nav-number">3.2.</span> <span class="nav-text">2. 2019-arXiv-ICML-Triple Distillation for Deep Face Recognition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2019-arXiv-Deep-Face-Recognition-Model-Compression-via-Knowledge-Transfer-and-Distillation"><span class="nav-number">3.3.</span> <span class="nav-text">3.2019-arXiv-Deep Face Recognition Model Compression via Knowledge Transfer and Distillation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#开源统计："><span class="nav-number">3.4.</span> <span class="nav-text">开源统计：</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xy</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
